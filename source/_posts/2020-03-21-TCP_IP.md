---
title:  "TCP/IP"
date:   2020-03-21 10:16:18 +0800
categories:
- TCP/IP
tags:
- TCP/IP 
- 网络编程 
- 服务器 


---

这篇文章主要记述了TCP/IP以及网络编程的知识。
<!--more-->


废话少说，首先，我们需要知道TCP在网络OSI的七层模型中的第四层——`Transport层`，IP在第三层——`Network层`，ARP在第二层——`Data Link层`，在第二层上的数据，我们叫`Frame`，在第三层上的数据叫`Packet`，第四层的数据叫`Segment`。



首先，我们需要知道，我们程序的数据首先会打到TCP的`Segment`中，然后TCP的`Segment`会打到IP的`Packet`中，然后再打到以太网`Ethernet`的`Frame`中，传到对端后，各个层解析自己的协议，然后把数据交给更高层的协议处理。

![image-20200403110519731](https://i.loli.net/2020/04/03/qOBXQYjWLgJhE2b.png)

## accept

`clntfd = accept(serfd,(struct sockaddr*)&servaddr,sizeof(clntaddr));`

`int accept(int sock,struct sockaddr* addr,socklen_t* addrlen);`

成功时返回创建的套接字文件描述符，失败时返回-1

为什么这里会再创建一个fd呢？

这是因为accept函数受理连接请求队列里面待处理的客户端的连接请求。函数调用成功后，accept函数内部会生成用于数据I/O的套接字，并返回其文件描述符。以下这幅图很形象的描述了这个过程：也就是说你客户端connect到我服务器listen的队列里，服务器有空了就从队列里取队头的客户端进行accept创建套接字进行连接。

它之所以被命名为套接字，就是说得配套，客户端人家是有connect给自动创建出来的fd，服务器先开始通过socket函数返回了文件描述符server_fd，这个是告诉进程可以通过像读写文件那样来进行网络通信

![image-20200401101317898](https://i.loli.net/2020/04/01/1gzbMQ6qr5VkNY7.png)

## Select

文件描述符的监视范围与第一个参数有关，实际上，select函数通过第一个参数来传递监视对象文件描述符的数量。因此，需要得到注册在fd_set变量中的文件描述符数量。但是每次新建文件描述符时，其值都会加1，故只需要将最大的文件描述符值加1再传递到select函数即可。加1是因为文件描述符的值从0开始。

```c
if((fd_num = select(fd_max+1,&cpy_reads,0,0,&timeout)) == -1)
    
for(int i = 0;i<fd_max+1;i++)
        {
            if(FD_ISSET(i,&cpy_reads))
            {
                if(i == servfd)
                {
                    clntfd = 
                     accept(i,(struct sockaddr*)&servaddr,sizeof(clntaddr));
                    FD_SET(clntfd,&reads);//reset 1
                    if(fd_max < clntfd)
                        fd_max = clntfd;//last fd occur
                    printf("connected! client :%d\n",clntfd);
                }
                else
                {
                    str_len = read(i,buf,BUF_SIZE);
                    if(str_len == 0)
                    {
                        FD_CLR(i,&reads);
                        close(i);
                        printf("closed client:%d \n",i);
                    }
                    else
                    {
                        write(i,buf,BUF_SIZE);
                    }                   
                    
                }
```

```
socklen_t clntsocklen = sizeof(clntaddr);
                    clntfd = accept(i,(struct sockaddr*)&servaddr,&clntsocklen);
```

请注意这两个代码段里面关于accept函数最后一个参数，它要求的是

`int accept(int __fd, struct sockaddr *__restrict__ __addr, socklen_t *__restrict__ __addr_len)`

其实是一个指向一个长度整数的地址。先开始我给传的是sizeof(clntaddr),但是调试时在这里返回-1.

## connection refused

服务器正常启动，但是客户端一启动就在`connect`函数退出了，错误显示`connection refused`，单步调试也看不出什么来。

上网一看，有的人也碰到了类似问题，他们怀疑出现这个状况最大的可能是服务器正在监听程序，但是客户端并没有按照规矩的IP和端口号来给服务器发程序：这不有的人就是因为把服务器的`serv_addr.sin_port = htons(atoi(argv[1]));`写错为`serv_addr.sin_port = htonl(atoi(argv[1]));`导致端口号分配出错了。

![image-20200401092200981](https://i.loli.net/2020/04/01/2UY1yrgvwzsNjD8.png)

当然我也好不到哪里去？在使用telnet localhost 9000发现服务器正常好用之后，我坚信我的客户端程序出了问题：一行一行代码排查，终于发现`serv_addr.sin_port = htons(argv[1]);` 我特么忘了加atoi，这端口号肯定是错了啊【初始端口为9000，16进制为0x2328经过hostToNetShort后变成0x2823–>10275】

![image-20200401100250744](https://i.loli.net/2020/04/01/OcltiYzaLw89gv3.png)

![image-20200401100638777](https://i.loli.net/2020/04/01/L6Qp4ORXPeFi5UY.png)





## Epoll

#### 数字电路里的边沿触发和电平触发

边沿触发包括上升边沿触发和下降边沿触发，边沿触发检测的是电平变化，高电平转低电平或低电平转高电平时，触发一次中断。

边沿触发通过D触发器来锁存中断信号，即：若CPU来不及响应中断，外部中断信号撤消后，由于D触发器的记忆作用，消失的中断信号仍然有效，直到中断被响应并进入中断ISR，记忆的中断信号才会由硬件自动清除。

-----

电平触发分为高电平触发和低电平触发；电平触发需要手动清除中断信号

电平触发根据硬件设计的不同，分为即时触发和信号锁存触发；

（1）即时的电平触发，当外部中断信号撤消时，中断申请信号随之消失。如果在外部中断信号申请期间，CPU来不及响应此中断，那么有可能这次中断申请就漏掉了。

即时的电平触发是一个时间段，需要一直触发中断的，就用电平触发。比如高电平触发，只要检测到是高电平就触发中断。

（2）信号锁存的电平触发，当检测到高电平或低电平信号，该触发信号也会被锁存，类似于边沿触发，但是触发信号需要进行手动清除；（注意前面的边沿触发会由硬件自动清除）

 

3、边沿触发及电平触发的区别

如果是采用边沿检测外部中断，检测到电平变化会中断，但是如果中断检测口一直保持某一电平，则无法产生下次中断，需要等下次检测到电平变化才会中断。中断得到响应后由硬件自动清除。

如果是采用电平检测外部中断，检测到低/高电平会中断，但是如果中断检测口一直保持低电平，中断处理完成后，会继续产生下次中断，需要检测到高电平才会停止中断产生。中断得到响应后由硬件手动清除。



![image-20200328232049698](https://i.loli.net/2020/03/30/4FkcyDpC9ZxMiTu.png)

![image-20200328232416075](https://i.loli.net/2020/03/30/IhUAb6fsQgp2LyG.png)

![image-20200328232423802](https://i.loli.net/2020/03/30/e7WKMlwZguRIyE4.png)

## 

epoll:产生的事件数；及消息；使用注意

epoll 全称 eventpoll，是 linux 内核实现IO多路复用（IO multiplexing）的一个实现

epoll 监听的fd(File Descriptor)集合是常驻内核的,它有3个系统调用(*epoll_create*, *epoll_wait*, epoll_ctl)通过 *epoll_wait* 可以多次监听同一个 fd 集合，只返回可读写那部分

select 只有一个系统调用，每次要监听都要将其从用户态传到内核，有事件时返回整个集合。

从性能上看，如果 fd 集合很大，用户态和内核态之间数据复制的花销是很大的，所以 select 一般限制 fd 集合最大1024。

从使用上看，epoll 返回的是可用的 fd 子集，select 返回的是全部，哪些可用需要用户遍历判断。

尽管如此，epoll 的性能并不必然比 select 高，对于 fd 数量较少并且 fd IO 都非常繁忙的情况 select 在性能上有优势。



## 不同socket地址结构对比

![image-20200401224452953](https://i.loli.net/2020/04/01/8ZcPnrdFHtVwW5D.png)



## 如何判断机器的大端与小端？

### 常见的字节序

一般操作系统都是小端，而通讯协议是大端的。

![image-20200401213715103](https://i.loli.net/2020/04/01/Nxpg5lwZLejzRTA.png)

```c
#define BigtoLittle16(A)  ((((uint16)(A) & 0xff00) >> 8) | 
							(((uint16)(A) & 0x00ff) << 8))
  
 
#define BigtoLittle32(A)  ((((uint32)(A) & 0xff000000) >> 24) |                                            						(((uint32)(A) & 0x00ff0000) >> 8) | \
                   (((uint32)(A) & 0x0000ff00) << 8) | \
                   (((uint32)(A) & 0x000000ff) << 24))
```



## 慢系统调用阻塞状态下来了个信号（Vital）

select函数、accept函数等都是慢系统调用函数，就是它得等系统网络IO准备好了才能执行，没有准备好就进入阻塞状态下，如果这个时候来了一个信号（因为我们知道信号是模仿的中断的嘛），当执行完信号处理函数返回后，PC指针就会跑到select函数的下一条指令去执行去了。。。。。。这不完蛋了吗：我等待的事件还没来呢我就被信号处理函数给跳过去了。

所以我们需要在select函数的返回值上做文章，如果select函数失败会返回-1，如果是去执行信号处理函数去了，error = EINTR

## 非阻塞编程

让服务器不会阻塞在send，accept等函数之上，防止有人恶意攻击

![image-20200322111836020](https://i.loli.net/2020/03/30/4txL8PY9qgjCdbJ.png)

![image-20200322112447217](https://i.loli.net/2020/03/30/JuRfAcSBvnHa1qr.png)

![image-20200322112833866](https://i.loli.net/2020/03/30/fHRLFJg8sGY6t4b.png)

![image-20200322113420519](https://i.loli.net/2020/03/30/M1fJEvI9H56tsWQ.png)

## IP头格式

![1576646164341](https://i.loli.net/2020/03/30/TyvP24FAzuBcQ9g.png)

![1576646175250](https://i.loli.net/2020/03/30/1LgXTeqvRxWmznf.png)



## TCP头格式（1460+20 = 1480）

### acknowledge(告知已收到)

接下来，我们来看一下TCP头的格式

![image-20200320091059508](https://i.loli.net/2020/03/30/amtJPn6jgFLT7BC.png)

逐个分析：

- 16位源端口号和16位目的端口号。
- 32位序号：一次TCP通信过程中某一个传输方向上的字节流的每个字节的编号，通过这个来确认发送的数据有序，比如现在序列号为1000，发送了1000，下一个序列号就是2000。
- 32位确认号：用来响应TCP报文段，给收到的TCP报文段的序号加1，三握时还要携带自己的序号。
- 4位头部长度：标识该TCP头部有多少个4字节，共表示最长15*4=60字节。同IP头部。
- 6位保留。6位标志。URG（紧急指针是否有效）ACK（表示确认号是否有效）PSH（提示接收端应用程序应该立即从TCP接收缓冲区读走数据）RST（表示要求对方重新建立连接）SYN（表示请求建立一个连接）FIN（表示通知对方本端要关闭连接）
- 16位窗口大小：TCP流量控制的一个手段，用来告诉对端TCP缓冲区还能容纳多少字节。
- 16位校验和：由发送端填充，接收端对报文段执行CRC算法以检验TCP报文段在传输中是否损坏。
- 16位紧急指针：一个正的偏移量，它和序号段的值相加表示最后一个紧急数据的下一字节的序号。

## 三次握手与四次挥手

![](https://i.loli.net/2020/03/30/uzFWcwyeg9NmIrZ.jpg)

![image-20200321103919380](https://i.loli.net/2020/03/30/9w3uJ4Iv8hYfcEC.png)



**握手过程可以简化为下面的四次交互：**

- 1 ) clien 端首先发送一个 SYN 包告诉 Server 端我的初始序列号是 X；
- 2 ) Server 端收到 SYN 包后回复给 client 一个 ACK 确认包，告诉 client 说我收到了；
- 3 ) 接着 Server 端也需要告诉 client 端自己的初始序列号，于是 Server 也发送一个 SYN 包告诉 client 我的初始序列号是Y；
- 4 ) Client 收到后，回复 Server 一个 ACK 确认包说我知道了。

#### 这里为什么服务器也要发一个初始序列号？

因为有可能客户端之前与服务器连过一次，后来网络断了，客户端再连接时你服务器好意思再从头给人家发一次吗？

**在三次握手过程中，细心的同学可能会有以下疑问：**

**问题1**：初始化序列号X、Y是可以是写死固定的吗，为什么不能呢？

如果初始化序列号（缩写为ISN：Inital Sequence Number）可以固定，我们来看看会出现什么问题：
假设ISN固定是1，Client和Server建立好一条TCP连接后，Client连续给Server发了10个包，这10个包不知怎么被链路上的路由器缓存了(路由器会毫无先兆地缓存或者丢弃任何的数据包)，这个时候碰巧Client挂掉了；
然后Client用同样的端口号重新连上Server，Client又连续给Server发了几个包，假设这个时候Client的序列号变成了5；
接着，之前被路由器缓存的10个数据包全部被路由到Server端了，Server给Client回复确认号10，这个时候，Client整个都不好了，这是什么情况？我的序列号才到5，你怎么给我的确认号是10了，整个都乱了。

seq的初始值是在创建连接时初始化的（值是随机的）

-----

**问题2**：假如Client发送一个SYN包给Server后就挂了或是不管了，这个时候这个连接处于什么状态呢？会超时吗？为什么呢？


Client发送SYN包给Server后挂了，Server回给Client的SYN-ACK一直没收到Client的ACK确认，这个时候这个连接既没建立起来，也不能算失败。这就需要一个超时时间让Server将这个连接断开，否则这个连接就会一直占用Server的SYN连接队列中的一个位置，大量这样的连接就会将Server的SYN连接队列耗尽，让正常的连接无法得到处理。

目前，Linux下默认会进行5次重发SYN-ACK包，重试的间隔时间从1s开始，下次的重试间隔时间是前一次的双倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了.所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会把断开这个连接。

由于，SYN超时需要63秒，那么就给攻击者一个攻击服务器的机会，攻击者在短时间内发送大量的SYN包给Server(俗称 SYN flood 攻击)，用于耗尽Server的SYN队列。对于应对SYN 过多的问题，linux提供了几个TCP参数：tcp_syncookies、tcp_synack_retries、tcp_max_syn_backlog、tcp_abort_on_overflow 来调整应对。

-----

**问题3**：四次挥手能不能变成三次挥手呢？


答案是可能的。

TCP是全双工通信，Cliet在自己已经不会在有新的数据要发送给Server后，可以发送FIN信号告知Server，这边已经终止Client到对端Server那边的数据传输。但是，这个时候对端Server可以继续往Client这边发送数据包。于是，两端数据传输的终止在时序上是独立并且可能会相隔比较长的时间，这个时候就必须最少需要2+2 = 4 次挥手来完全终止这个连接。但是，如果Server在收到Client的FIN包后，在也没数据需要发送给Client了，那么对Client的ACK包和Server自己的FIN包就可以合并成为一个包发送过去，这样四次挥手就可以变成三次了(似乎linux协议栈就是这样实现的)。

TCP进行断开连接的目标是：回收资源、终止数据传输。由于TCP是全双工的，需要Peer两端分别各自拆除自己通向Peer对端的方向的通信信道。

**这样需要四次挥手来分别拆除通信信道，就比较清晰明了了：**

- 1）Client 发送一个FIN包来告诉 Server 我已经没数据需要发给 Server了；

- 2）Server 收到后回复一个 ACK 确认包说我知道了；

- 3）然后 server 在自己也没数据发送给client后，Server 也发送一个 FIN 包给 Client 告诉 Client 我也已经没数据发给client 了；

- 4）Client 收到后，就会回复一个 ACK 确认包说我知道了。

  -----

**问题4：**很多人会问，为什么建链接要3次握手，断链接需要4次挥手？

- **对于建链接的3次握手，**主要是要初始化Sequence Number 的初始值。通信的双方要互相通知对方自己的初始化的Sequence Number（缩写为ISN：Inital Sequence Number）——所以叫SYN，全称Synchronize Sequence Numbers。也就上图中的 x 和 y。这个号要作为以后的数据通信的序号，以保证应用层接收到的数据不会因为网络上的传输的问题而乱序（TCP会用这个序号来拼接数据）。
- **对于4次挥手，**其实你仔细看是2次，因为TCP是全双工的，所以，发送方和接收方都需要Fin和Ack。只不过，有一方是被动的，所以看上去就成了所谓的4次挥手。如果两边同时断连接，那就会就进入到CLOSING状态，然后到达TIME_WAIT状态。下图是双方同时断连接的示意图（你同样可以对照着TCP状态机看）：

不知道大家有没有注意到客户端在发送了最后一个断开请求的ACK后，又等待了2MSL的时间才关闭连接。为什么不直接关闭连接呢？如果客户端直接关闭连接，而此时客户端最后发送的ACK又在网络中丢失，从而可能导致服务器端的连接无法正常关闭。那为什么又要设置为2MSL呢？1MSL表示一个IP数据报在网络中的最多存活时间。假设客户端最后发送的ACK经过将近1MSL快要到达服务器端的时候丢失了，那么服务器端在规定的时间内未收到最后客户端发送的ACK，则服务器端重新发送最后的FIN给客户端，请求客户端重发ACK，该FIN经过1MSL到达客户端。所以如上最坏情况，如果客户端在2MSL内没有收到FIN请求，则表明服务器端已经断开连接。

另外，有几个事情需要注意一下：

- **关于建连接时SYN超时**。试想一下，如果server端接到了client发的SYN后回了SYN-ACK后client掉线了，server端没有收到client回来的ACK，那么，这个连接处于一个中间状态，即没成功，也没失败。于是，server端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻售，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 2^6 -1 = 63s，TCP才会把断开这个连接。
- **关于SYN Flood攻击**。一些恶意的人就为此制造了SYN Flood攻击——给服务器发了一个SYN后，就下线了，于是服务器需要默认等63s才会断开连接，这样，攻击者就可以把服务器的syn连接的队列耗尽，让正常的连接请求不能处理。于是，Linux下给了一个叫**tcp_syncookies**的参数来应对这个事——当SYN队列满了后，TCP会通过源地址端口、目标地址端口和时间戳打造出一个特别的Sequence Number发回去（又叫cookie），如果是攻击者则不会有响应，如果是正常连接，则会把这个 SYN Cookie发回来，然后服务端可以通过cookie建连接（即使你不在SYN队列中）。请注意，**请先千万别用tcp_syncookies来处理正常的大负载的连接的情况**。因为，synccookies是妥协版的TCP协议，并不严谨。对于正常的请求，你应该调整三个TCP参数可供你选择，第一个是：tcp_synack_retries 可以用他来减少重试次数；第二个是：tcp_max_syn_backlog，可以增大SYN连接数；第三个是：tcp_abort_on_overflow 处理不过来干脆就直接拒绝连接了。
- **关于ISN的初始化**。ISN是不能hard code的，不然会出问题的——比如：如果连接建好后始终用1来做ISN，如果client发了30个segment过去，但是网络断了，于是 client重连，又用了1做ISN，但是之前连接的那些包到了，于是就被当成了新连接的包，此时，client的Sequence Number 可能是3，而Server端认为client端的这个号是30了。全乱了。[RFC793](http://tools.ietf.org/html/rfc793)中说，ISN会和一个假的时钟绑在一起，这个时钟会在每4微秒对ISN做加一操作，直到超过2^32，又从0开始。这样，一个ISN的周期大约是4.55个小时。因为，我们假设我们的TCP Segment在网络上的存活时间不会超过Maximum Segment Lifetime（缩写为MSL – [Wikipedia语条](http://en.wikipedia.org/wiki/Maximum_Segment_Lifetime)），所以，只要MSL的值小于4.55小时，那么，我们就不会重用到ISN。
- **关于 MSL 和 TIME_WAIT**。通过上面的ISN的描述，相信你也知道MSL是怎么来的了。我们注意到，在TCP的状态图中，从TIME_WAIT状态到CLOSED状态，有一个超时设置，这个超时设置是 2*MSL（[RFC793](http://tools.ietf.org/html/rfc793)定义了MSL为2分钟，Linux设置成了30s）为什么要这有TIME_WAIT？为什么不直接给转成CLOSED状态呢？主要有两个原因：1）TIME_WAIT确保有足够的时间让对端收到了ACK，如果被动关闭的那方没有收到Ack，就会触发被动端重发Fin，一来一去正好2个MSL，2）有足够的时间让这个连接不会跟后面的连接混在一起（你要知道，有些自做主张的路由器会缓存IP数据包，如果连接被重用了，那么这些延迟收到的包就有可能会跟新连接混在一起）。你可以看看这篇文章《[TIME_WAIT and its design implications for protocols and scalable client server systems](http://www.serverframework.com/asynchronousevents/2011/01/time-wait-and-its-design-implications-for-protocols-and-scalable-servers.html)》
- **关于TIME_WAIT数量太多**。从上面的描述我们可以知道，TIME_WAIT是个很重要的状态，但是如果在大并发的短链接下，TIME_WAIT 就会太多，这也会消耗很多系统资源。只要搜一下，你就会发现，十有八九的处理方式都是教你设置两个参数，一个叫**tcp_tw_reuse**，另一个叫**tcp_tw_recycle**的参数，这两个参数默认值都是被关闭的，后者recyle比前者resue更为激进，resue要温柔一些。另外，如果使用tcp_tw_reuse，必需设置tcp_timestamps=1，否则无效。这里，你一定要注意，**打开这两个参数会有比较大的坑——可能会让TCP连接出一些诡异的问题**（因为如上述一样，如果不等待超时重用连接的话，新的连接可能会建不上。正如[官方文档](https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt)上说的一样“**It should not be changed without advice/request of technical experts**”）。
- **关于tcp_tw_reuse**。官方文档上说tcp_tw_reuse 加上tcp_timestamps（又叫PAWS, for Protection Against Wrapped Sequence Numbers）可以保证协议的角度上的安全，但是你需要tcp_timestamps在两边都被打开（你可以读一下[tcp_twsk_unique](http://lxr.free-electrons.com/ident?i=tcp_twsk_unique)的源码 ）。我个人估计还是有一些场景会有问题。
- **关于tcp_tw_recycle**。如果是tcp_tw_recycle被打开了话，会假设对端开启了tcp_timestamps，然后会去比较时间戳，如果时间戳变大了，就可以重用。但是，如果对端是一个NAT网络的话（如：一个公司只用一个IP出公网）或是对端的IP被另一台重用了，这个事就复杂了。建链接的SYN可能就被直接丢掉了（你可能会看到connection time out的错误）（如果你想观摩一下Linux的内核代码，请参看源码[ tcp_timewait_state_process](http://lxr.free-electrons.com/ident?i=tcp_timewait_state_process)）。
- **关于tcp_max_tw_buckets**。这个是控制并发的TIME_WAIT的数量，默认值是180000，如果超限，那么，系统会把多的给destory掉，然后在日志里打一个警告（如：time wait bucket table overflow），官网文档说这个参数是用来对抗DDoS攻击的。也说的默认值180000并不小。这个还是需要根据实际情况考虑。

## ![image-20200415213059169](https://i.loli.net/2020/04/15/eEIryOlknCYH6tq.png)



## 流量控制

流量控制是一个端到端的问题。

## 拥塞控制

拥塞控制是一个全局性的过程。涉及到所有的主机、路由器等。拥塞控制和流量控制之所以常常被弄混，是因为某些拥塞控制算法是向发送端发送控制报文，并告诉发送端，网络已出现麻烦，必须放慢发送速率。这点又和流量控制是很相似的。

TCP进行拥塞控制的四种算法：慢开始，拥塞避免，快重传和快恢复。

下面讨论的拥塞控制基于窗口。为此，发送方维持一个叫做拥塞窗口的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态的在变化。发送方让自己的发送窗口等于拥塞窗口。

发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就可以再增大一点，以便把更多的分组发送出去，这样可以提高网络的利用率。但是只要网络中出现拥塞，就必须把拥塞窗口减小一点，以减少注入到网络中的分组数，以便缓解网络出现的阻塞。

### 慢开始

服务器发送数据包，当然越快越好，最好一次性全发出去。但是，发得太快，就有可能丢包。带宽小、路由器过热、缓存溢出等许多因素都会导致丢包。线路不好的话，发得越快，丢得越多。

最理想的状态是，在线路允许的情况下，达到最高速率。但是我们怎么知道，对方线路的理想速率是多少呢？答案就是慢慢试。

TCP 协议为了做到效率与可靠性的统一，设计了一个慢启动（slow start）机制。开始的时候，发送得较慢，然后根据丢包的情况，调整速率：如果不丢包，就加快发送速度；如果丢包，就降低发送速度。

Linux 内核里面[设定](http://elixir.free-electrons.com/linux/v4.5/source/include/net/tcp.h#L220)了（常量`TCP_INIT_CWND`），刚开始通信的时候，发送方一次性发送10个数据包，即"发送窗口"的大小为10。然后停下来，等待接收方的确认，再继续发送。

默认情况下，接收方每收到[两个](https://serverfault.com/questions/348666/when-the-tcp-engine-decides-to-send-an-ack) TCP 数据包，就要[发送](https://stackoverflow.com/a/3604882/1194049)一个确认消息。"确认"的英语是 acknowledgement，所以这个确认消息就简称 ACK。

ACK 携带两个信息。

> - 期待要收到下一个数据包的编号
> - 接收方的接收窗口的剩余容量

发送方有了这两个信息，再加上自己已经发出的数据包的最新编号，就会推测出接收方大概的接收速度，从而降低或增加发送速率。这被称为"发送窗口"，这个窗口的大小是可变的。

快送重传就是基于以下机制：如果假设重复阈值为3，当发送方收到4次相同确认号的分段确认（第1次收到确认期望序列号，加3次重复的期望序列号确认）时，则可以认为继续发送更高序列号的分段将会被接受方丢弃，而且会无法有序送达。发送方应该忽略超时计时器的等待重发，立即重发重复分段确认中确认号对应序列号的分段。

![image-20200415213302421](https://i.loli.net/2020/04/15/mGzK1xJvgqbfURn.png)

# Reactor

最最原始的网络编程思路就是服务器用一个while循环，不断监听端口是否有新的套接字连接，如果有，那么就调用一个处理函数处理，类似：
while(true){
socket = accept();
handle(socket)
}
这种方法的最大问题是无法并发，效率太低，如果当前的请求没有处理完，那么后面的请求只能被阻塞，服务器的吞吐量太低。
之后，想到了使用多线程，也就是很经典的connection per thread，每一个连接用一个线程处理，类似：
while(true){
socket = accept();
new thread(socket);
}
tomcat服务器的早期版本确实是这样实现的。多线程的方式确实一定程度上极大地提高了服务器的吞吐量，因为之前的请求在read阻塞以后，不会影响到后续的请求，因为他们在不同的线程中。这也是为什么通常会讲“一个线程只能对应一个socket”的原因。最开始对这句话很不理解，线程中创建多个socket不行吗？语法上确实可以，但是实际上没有用，每一个socket都是阻塞的，所以在一个线程里只能处理一个socket，就算accept了多个也没用，前一个socket被阻塞了，后面的是无法被执行到的。
缺点在于资源要求太高，系统中创建线程是需要比较高的系统资源的，如果连接数太高，系统无法承受，而且，线程的反复创建-销毁也需要代价。
线程池本身可以缓解线程创建-销毁的代价，这样优化确实会好很多，不过还是存在一些问题的，就是线程的粒度太大。每一个线程把一次交互的事情全部做了，包括读取和返回，甚至连接，表面上似乎连接不在线程里，但是如果线程不够，有了新的连接，也无法得到处理，所以，目前的方案线程里可以看成要做三件事，连接，读取和写入。
线程同步的粒度太大了，限制了吞吐量。应该把一次连接的操作分为更细的粒度或者过程，这些更细的粒度是更小的线程。整个线程池的数目会翻倍，但是线程更简单，任务更加单一。这其实就是Reactor出现的原因，**在Reactor中，这些被拆分的小线程或者子过程对应的是handler，每一种handler会出处理一种event。这里会有一个全局的管理者selector，我们需要把channel注册感兴趣的事件，那么这个selector就会不断在channel上检测是否有该类型的事件发生，如果没有，那么主线程就会被阻塞，否则就会调用相应的事件处理函数即handler来处理。**典型的事件有连接，读取和写入，当然我们就需要为这些事件分别提供处理器，每一个处理器可以采用线程的方式实现。一个连接来了，显示被读取线程或者handler处理了，然后再执行写入，那么之前的读取就可以被后面的请求复用，吞吐量就提高了。

几乎所有的网络连接都会经过读请求内容——》解码——》计算处理——》编码回复——》回复的过程，Reactor模式的的演化过程如下：

![image-20200406221245258](https://i.loli.net/2020/04/06/7jhTwgWozZEFAqi.png)

这种模型由于IO在阻塞时会一直等待，因此在用户负载增加时，性能下降的非常快。

server导致阻塞的原因：

1、serversocket的accept方法，阻塞等待client连接，直到client连接成功。

2、线程从socket inputstream读入数据，会进入阻塞状态，直到全部数据读完。

3、线程向socket outputstream写入数据，会阻塞直到全部数据写完。