---
title:  "GPU并行编程--CUDA模型学习"
date:   2020-03-08 10:16:18 +0800
categories:
- C++
tags: 
- CUDA 
- GPU 


---

这篇文章主要记述了GPU并行编程CUDA的一些笔记。
<!--more-->






![1583476702566](https://i.loli.net/2020/03/30/HtDAjOmXqVnLKgb.png)

32是考虑流水线【取指--执行----】

![1583478205636](https://i.loli.net/2020/03/30/Lp1crIB4K5EmHQX.png)

![1583480186631](https://i.loli.net/2020/03/30/gJCWhvV6HkwqTd2.png)

均匀分布比起高斯分布具有更高的信息熵（更混乱，因为均匀分布说明物体很多且分布均匀，而高斯分布集中在峰值那块，更整齐一些）

![1583995920661](https://i.loli.net/2020/03/30/lY8G7mgMEnroPa2.png)

![1583996284858](https://i.loli.net/2020/03/30/zXL9xRYIdmP6SEO.png)

第二个式子是保证正负号的，也就是保证分类正确（因为求w最小求出两个值）

![1583998459306](https://i.loli.net/2020/03/30/gMdYEFTuqoCByi1.png)

可以使用 `nvidia-smi` (*Systems Management Interface*) 命令行命令查询有关此 GPU 的信息。

![1584235758864](https://i.loli.net/2020/03/30/RbHi3vYFqhWt7yK.png)

可以通过以下方式轻松解决这种情况：

- 编写执行配置，使其创建的线程数**超过**执行分配工作所需的线程数。
- 将值作为参数传递到核函数 (`N`) 中，以表示要处理的数据集总大小或完成工作所需的总线程数。
- 计算网格内的线程索引后（使用 `tid+bid*bdim`），请检查该索引是否超过 `N`，并且只在不超过的情况下执行与核函数相关的工作。

以下是编写执行配置的惯用方法示例，适用于 `N` 和线程块中的线程数已知，但无法保证网格中的线程数和 `N` 之间完全匹配的情况。如此一来，便可确保网格中至少始终拥有 `N` 所需的线程数，且超出的线程数至多仅可相当于 1 个线程块的线程数量：

```cpp
// Assume `N` is known
int N = 100000;

// Assume we have a desire to set `threads_per_block` exactly to `256`
size_t threads_per_block = 256;

// Ensure there are at least `N` threads in the grid, but only 1 block's worth extra
size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;

some_kernel<<<number_of_blocks, threads_per_block>>>(N);
```

由于上述执行配置致使网格中的线程数超过 `N`，因此需要注意 `some_kernel` 定义中的内容，以确保 `some_kernel` 在由其中一个 \”extra\” 线程执行时不会尝试访问超出范围的数据元素：

```cpp
__global__ some_kernel(int N)
{
  int idx = threadIdx.x + blockIdx.x * blockDim.x;

  if (idx < N) // Check to make sure `idx` maps to some value within `N`
  {
    // Only do work if it does
  }
}
```

![1584163725409](https://i.loli.net/2020/03/30/pEVzKnYhoGvtjOe.png)

---
## Allocating Memory to be accessed on the GPU and the CPU

CUDA 的最新版本（版本 6 和更高版本）已能轻松分配可用于 CPU 主机和任意数量 GPU 设备的内存。尽管现今有许多适用于内存管理并可支持加速应用程序中最优性能的 [中高级技术](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)，但我们现在要介绍的基础 CUDA 内存管理技术不但能够支持远超 CPU 应用程序的卓越性能，而且几乎不会产生任何开发人员成本。

如要分配和释放内存，并获取可在主机和设备代码中引用的指针，请使用 `cudaMallocManaged` 和 `cudaFree` 取代对 `malloc` 和 `free` 的调用，如下例所示：

```cpp
// CPU-only

int N = 2<<20;
size_t size = N * sizeof(int);

int *a;
a = (int *)malloc(size);

// Use `a` in CPU-only program.

free(a);
```

```cpp
// Accelerated

int N = 2<<20;
size_t size = N * sizeof(int);

int *a;
// Note the address of `a` is passed as first argument.
cudaMallocManaged(&a, size);

// Use `a` on the CPU and/or on any GPU in the accelerated system.

cudaFree(a);
```

## Data Sets Larger then the Grid

或出于选择，为了要创建具有超高性能的执行配置，或出于需要，一个网格中的线程数量可能会小于数据集的大小。请思考一下包含 1000 个元素的数组和包含 250 个线程的网格（此处使用极小的规模以便于说明）。此网格中的每个线程将需使用 4 次。如要实现此操作，一种常用方法便是在核函数中使用**网格跨度循环**。

在网格跨度循环中，每个线程将在网格内使用 `tid+bid*bdim` 计算自身唯一的索引，并对数组内该索引的元素执行相应运算，然后将网格中的线程数添加到索引并重复此操作，直至超出数组范围。例如，对于包含 500 个元素的数组和包含 250 个线程的网格，网格中索引为 20 的线程将执行如下操作：

- 对包含 500 个元素的数组的元素 20 执行相应运算
- 将其索引增加 250，使网格的大小达到 270
- 对包含 500 个元素的数组的元素 270 执行相应运算
- 将其索引增加 250，使网格的大小达到 520
- 由于 520 现已超出数组范围，因此线程将停止工作

CUDA 提供一个可给出网格中线程块数的特殊变量：`gridDim.x`。然后计算网格中的总线程数，即网格中的线程块数乘以每个线程块中的线程数：`gridDim.x * blockDim.x`。带着这样的想法来看看以下核函数中网格跨度循环的详细示例：

```cpp
__global void kernel(int *a, int N)
{
  int indexWithinTheGrid = threadIdx.x + blockIdx.x * blockDim.x;
  int gridStride = gridDim.x * blockDim.x;

  for (int i = indexWithinTheGrid; i < N; i += gridStride)
  {
    // do work on a[i];
  }
}
```

## Error Handling

与在任何应用程序中一样，加速 CUDA 代码中的错误处理同样至关重要。即便不是大多数，也有许多 CUDA 函数（例如，[内存管理函数](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY)）会返回类型为 `cudaError_t` 的值，该值可用于检查调用函数时是否发生错误。以下是对调用 `cudaMallocManaged` 函数执行错误处理的示例：

```cpp
cudaError_t err;
err = cudaMallocManaged(&a, N)                    // Assume the existence of `a` and `N`.

if (err != cudaSuccess)                           // `cudaSuccess` is provided by CUDA.
{
  printf("Error: %s\n", cudaGetErrorString(err)); // `cudaGetErrorString` is provided by CUDA.
}
```

启动定义为返回 `void` 的核函数后，将不会返回类型为 `cudaError_t` 的值。为检查启动核函数时是否发生错误（例如，如果启动配置错误），CUDA 提供 `cudaGetLastError` 函数，该函数会返回类型为 `cudaError_t` 的值。

```cpp
/*
 * This launch should cause an error, but the kernel itself
 * cannot return it.
 */

someKernel<<<1, -1>>>();  // -1 is not a valid number of threads.

cudaError_t err;
err = cudaGetLastError(); // `cudaGetLastError` will return the error from above.
if (err != cudaSuccess)
{
  printf("Error: %s\n", cudaGetErrorString(err));
}
```

最后，为捕捉异步错误（例如，在异步核函数执行期间），请务必检查后续同步 CUDA 运行时 API 调用所返回的状态（例如 `cudaDeviceSynchronize`）；如果之前启动的其中一个核函数失败，则将返回错误。

### CUDA Error Handling Function

创建一个包装 CUDA 函数调用的宏对于检查错误十分有用。以下是一个宏示例，您可以在余下练习中随时使用：

```cpp
#include <stdio.h>
#include <assert.h>

inline cudaError_t checkCuda(cudaError_t result)
{
  if (result != cudaSuccess) {
    fprintf(stderr, "CUDA Runtime Error: %s\n", cudaGetErrorString(result));
    assert(result == cudaSuccess);
  }
  return result;
}

int main()
{

/*
 * The macro can be wrapped around any function returning
 * a value of type `cudaError_t`.
 */

  checkCuda( cudaDeviceSynchronize() )
}
```

### Final Exercise: Accelerate Vector Addition Application

下面的挑战将使您有机会运用在实验中学到的知识。其中涉及加速 CPU 向量加法程序，尽管该程序不甚复杂，但仍能让您有机会重点运用所学的借助 CUDA 加速 GPU 应用程序的相关知识。完成此练习后，如果您有富余时间并有意深究，可继续学习*高阶内容*部分以了解涉及更复杂代码库的一些挑战。

01-vector-add.cu包含一个可正常运作的 CPU 向量加法应用程序。加速其 `addVectorsInto` 函数，使之在 GPU 上以 CUDA 核函数运行并使其并行执行工作。鉴于需发生以下操作，如您遇到问题，请参阅解决方案。

- 扩充 `addVectorsInto` 定义，使之成为 CUDA 核函数。
- 选择并使用有效的执行配置，以使 `addVectorsInto` 作为 CUDA 核函数启动。
- 更新内存分配，内存释放以反映主机和设备代码需要访问 3 个向量：`a`、`b` 和 `result`。
- 重构 `addVectorsInto` 的主体：将在单个线程内部启动，并且只需对输入向量执行单线程操作。确保线程从不尝试访问输入向量范围之外的元素，并注意线程是否需对输入向量的多个元素执行操作。
- 在 CUDA 代码可能以其他方式静默失败的位置添加错误处理。

```cpp
#include <stdio.h>
#include <assert.h>
inline cudaError_t checkCuda(cudaError_t result)
{
  if (result != cudaSuccess) {
    fprintf(stderr, "CUDA Runtime Error: %s\n", cudaGetErrorString(result));
    assert(result == cudaSuccess);
  }
  return result;
}
void initWith(float num, float *a, int N)
{
  for(int i = 0; i < N; ++i)
  {
    a[i] = num;
  }
}

__global__ void addVectorsInto(float *result, float *a, float *b, int N)
{
    int idx = threadIdx.x + blockIdx.x * blockDim.x;    
    int gridStride = gridDim.x * blockDim.x;
    
    for(int i = idx;i<N;i+=gridStride){
          for(int i = 0; i < N; ++i)
          {
            result[i] = a[i] + b[i];
          }
      }
}

void checkElementsAre(float target, float *array, int N)
{
  for(int i = 0; i < N; i++)
  {
    if(array[i] != target)
    {
      printf("FAIL: array[%d] - %0.0f does not equal %0.0f\n", i, array[i], target);
      exit(1);
    }
  }
  printf("SUCCESS! All values added correctly.\n");
}

int main()
{
  const int N = 2<<20;
  size_t size = N * sizeof(float);

  float *a;
  float *b;
  float *c;

 
  cudaMallocManaged(&a,size);
  cudaMallocManaged(&b,size);
  cudaMallocManaged(&c,size);
 

  initWith(3, a, N);
  initWith(4, b, N);
  initWith(0, c, N);

  addVectorsInto<<<100,100>>>(c, a, b, N);
  //cudaDeviceSynchronize();
  checkCuda( cudaDeviceSynchronize() );

  checkElementsAre(7, c, N);

  cudaFree(a);
  cudaFree(b);
  cudaFree(c);
}

```

## Grids and Blocks of 2 and 3 Dimensions

可以将网格和线程块定义为最多具有 3 个维度。使用多个维度定义网格和线程块绝不会对其性能造成任何影响，但这在处理具有多个维度的数据时可能非常有用，例如 2D 矩阵。如要定义二维或三维网格或线程块，可以使用 CUDA 的 `dim3` 类型，即如下所示：

```cpp
dim3 threads_per_block(16, 16, 1);
dim3 number_of_blocks(16, 16, 1);
someKernel<<<number_of_blocks, threads_per_block>>>();
```

鉴于以上示例，`someKernel` 内部的变量 `gridDim.x`、`gridDim.y`、`blockDim.x` 和 `blockDim.y` 均将等于 `16`。

### Exercise: Accelerate 2D Matrix Multiply Application

文件 [`01-matrix-multiply-2d.cu`](http://ec2-3-89-85-142.compute-1.amazonaws.com/DdJxiLJo/edit/tasks/task1/task/01_AC_CUDA_C-zh/08-matrix-multiply/01-matrix-multiply-2d.cu) 包含一个功能齐全的主机函数 `matrixMulCPU`。您的任务是扩建 CUDA 核函数 `matrixMulGPU`。源代码将使用这两个函数执行矩阵乘法，并比较它们的答案以验证您编写的 CUDA 核函数是否正确。使用以下指南获得操作支持，如您遇到问题，请参阅 [解决方案](http://ec2-3-89-85-142.compute-1.amazonaws.com/DdJxiLJo/edit/tasks/task1/task/01_AC_CUDA_C-zh/08-matrix-multiply/solutions/01-matrix-multiply-2d-solution.cu)：

- 您将需创建执行配置，其参数均为 `dim3` 值，且 `x` 和 `y` 维度均设为大于 `1`。
- 在核函数主体内部，您将需要按照惯例在网格内建立所运行线程的唯一索引，但应为线程建立两个索引：一个用于网格的 x 轴，另一个用于网格的 y 轴。

```c
#include <stdio.h>

#define N  64

__global__ void matrixMulGPU( int * a, int * b, int * c )
{
  int val = 0;

  int row = blockIdx.x * blockDim.x + threadIdx.x;
  int col = blockIdx.y * blockDim.y + threadIdx.y;

  if (row < N && col < N)
  {
    for ( int k = 0; k < N; ++k )
      val += a[row * N + k] * b[k * N + col];
    c[row * N + col] = val;
  }
   
}

/*
 * This CPU function already works, and will run to create a solution matrix
 * against which to verify your work building out the matrixMulGPU kernel.
 */

void matrixMulCPU( int * a, int * b, int * c )
{
  int val = 0;

  for( int row = 0; row < N; ++row )
    for( int col = 0; col < N; ++col )
    {
      val = 0;
      for ( int k = 0; k < N; ++k )
        val += a[row * N + k] * b[k * N + col];
      c[row * N + col] = val;
    }
}

int main()
{
  int *a, *b, *c_cpu, *c_gpu; // Allocate a solution matrix for both the CPU and the GPU operations

  int size = N * N * sizeof (int); // Number of bytes of an N x N matrix

  // Allocate memory
  cudaMallocManaged (&a, size);
  cudaMallocManaged (&b, size);
  cudaMallocManaged (&c_cpu, size);
  cudaMallocManaged (&c_gpu, size);

  // Initialize memory; create 2D matrices
  for( int row = 0; row < N; ++row )
    for( int col = 0; col < N; ++col )
    {
      a[row*N + col] = row;
      b[row*N + col] = col+2;
      c_cpu[row*N + col] = 0;
      c_gpu[row*N + col] = 0;
    }

  /*
   * Assign `threads_per_block` and `number_of_blocks` 2D values
   * that can be used in matrixMulGPU above.
   */

  dim3 threads_per_block(16,16,1);
  dim3 number_of_blocks(4,4,1);

  matrixMulGPU <<< number_of_blocks, threads_per_block >>> ( a, b, c_gpu );

  cudaDeviceSynchronize();

  // Call the CPU version to check our work
  matrixMulCPU( a, b, c_cpu );

  // Compare the two answers to make sure they are equal
  bool error = false;
  for( int row = 0; row < N && !error; ++row )
    for( int col = 0; col < N && !error; ++col )
      if (c_cpu[row * N + col] != c_gpu[row * N + col])
      {
        printf("FOUND ERROR at c[%d][%d]\n", row, col);
        error = true;
        break;
      }
  if (!error)
    printf("Success!\n");

  // Free all our allocated memory
  cudaFree(a); cudaFree(b);
  cudaFree( c_cpu ); cudaFree( c_gpu );
}

```

### Exercise: Accelerate A Thermal Conductivity Application

在下面的练习中，您将为模拟金属银二维热传导的应用程序执行加速操作。

将 [`01-heat-conduction.cu`](http://ec2-3-84-208-168.compute-1.amazonaws.com/3riRC33t/edit/tasks/task1/task/01_AC_CUDA_C-zh/09-heat/01-heat-conduction.cu) 内的 `step_kernel_mod` 函数转换为在 GPU 上执行，并修改 `main` 函数以恰当分配在 CPU 和 GPU 上使用的数据。`step_kernel_ref` 函数在 CPU 上执行并用于检查错误。由于此代码涉及浮点计算，因此不同的处理器甚或同一处理器上的简单重排操作都可能导致结果略有出入。为此，错误检查代码会使用错误阈值，而非查找完全匹配。如您遇到问题，请参阅 [解决方案](http://ec2-3-84-208-168.compute-1.amazonaws.com/3riRC33t/edit/tasks/task1/task/01_AC_CUDA_C-zh/09-heat/solutions/01-heat-conduction-solution.cu)。

```c
#include <stdio.h>
#include <math.h>

// Simple define to index into a 1D array from 2D space
#define I2D(num, c, r) ((r)*(num)+(c))

__global__
void step_kernel_mod(int ni, int nj, float fact, float* temp_in, float* temp_out)
{
  int i00, im10, ip10, i0m1, i0p1;
  float d2tdx2, d2tdy2;

  int j = blockIdx.x * blockDim.x + threadIdx.x;
  int i = blockIdx.y * blockDim.y + threadIdx.y;

  // loop over all points in domain (except boundary)
  if (j > 0 && i > 0 && j < nj-1 && i < ni-1) {
    // find indices into linear memory
    // for central point and neighbours
    i00 = I2D(ni, i, j);
    im10 = I2D(ni, i-1, j);
    ip10 = I2D(ni, i+1, j);
    i0m1 = I2D(ni, i, j-1);
    i0p1 = I2D(ni, i, j+1);

    // evaluate derivatives
    d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10];
    d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1];

    // update temperatures
    temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2);
  }
}

void step_kernel_ref(int ni, int nj, float fact, float* temp_in, float* temp_out)
{
  int i00, im10, ip10, i0m1, i0p1;
  float d2tdx2, d2tdy2;


  // loop over all points in domain (except boundary)
  for ( int j=1; j < nj-1; j++ ) {
    for ( int i=1; i < ni-1; i++ ) {
      // find indices into linear memory
      // for central point and neighbours
      i00 = I2D(ni, i, j);
      im10 = I2D(ni, i-1, j);
      ip10 = I2D(ni, i+1, j);
      i0m1 = I2D(ni, i, j-1);
      i0p1 = I2D(ni, i, j+1);

      // evaluate derivatives
      d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10];
      d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1];

      // update temperatures
      temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2);
    }
  }
}

int main()
{
  int istep;
  int nstep = 200; // number of time steps

  // Specify our 2D dimensions
  const int ni = 200;
  const int nj = 100;
  float tfac = 8.418e-5; // thermal diffusivity of silver

  float *temp1_ref, *temp2_ref, *temp1, *temp2, *temp_tmp;

  const int size = ni * nj * sizeof(float);

  temp1_ref = (float*)malloc(size);
  temp2_ref = (float*)malloc(size);
  cudaMallocManaged(&temp1, size);
  cudaMallocManaged(&temp2, size);

  // Initialize with random data
  for( int i = 0; i < ni*nj; ++i) {
    temp1_ref[i] = temp2_ref[i] = temp1[i] = temp2[i] = (float)rand()/(float)(RAND_MAX/100.0f);
  }

  // Execute the CPU-only reference version
  for (istep=0; istep < nstep; istep++) {
    step_kernel_ref(ni, nj, tfac, temp1_ref, temp2_ref);

    // swap the temperature pointers
    temp_tmp = temp1_ref;
    temp1_ref = temp2_ref;
    temp2_ref= temp_tmp;
  }

  dim3 tblocks(32, 16, 1);
  dim3 grid((nj/tblocks.x)+1, (ni/tblocks.y)+1, 1);
  cudaError_t ierrSync, ierrAsync;

  // Execute the modified version using same data
  for (istep=0; istep < nstep; istep++) {
    step_kernel_mod<<< grid, tblocks >>>(ni, nj, tfac, temp1, temp2);

    ierrSync = cudaGetLastError();
    ierrAsync = cudaDeviceSynchronize(); // Wait for the GPU to finish
    if (ierrSync != cudaSuccess) { printf("Sync error: %s\n", cudaGetErrorString(ierrSync)); }
    if (ierrAsync != cudaSuccess) { printf("Async error: %s\n", cudaGetErrorString(ierrAsync)); }

    // swap the temperature pointers
    temp_tmp = temp1;
    temp1 = temp2;
    temp2= temp_tmp;
  }

  float maxError = 0;
  // Output should always be stored in the temp1 and temp1_ref at this point
  for( int i = 0; i < ni*nj; ++i ) {
    if (abs(temp1[i]-temp1_ref[i]) > maxError) { maxError = abs(temp1[i]-temp1_ref[i]); }
  }

  // Check and see if our maxError is greater than an error bound
  if (maxError > 0.0005f)
    printf("Problem! The Max Error of %.5f is NOT within acceptable bounds.\n", maxError);
  else
    printf("The Max Error of %.5f is within acceptable bounds.\n", maxError);

  free( temp1_ref );
  free( temp2_ref );
  cudaFree( temp1 );
  cudaFree( temp2 );

  return 0;
}

```

```
==188== NVPROF is profiling process 188, command: ./single-thread-vector-add
Success! All values calculated correctly.
==188== Profiling application: ./single-thread-vector-add
==188== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:  100.00%  2.37011s         1  2.37011s  2.37011s  2.37011s  addVectorsInto(float*, float*, float*, int)
      API calls:   57.28%  2.37013s         1  2.37013s  2.37013s  2.37013s  cudaDeviceSynchronize
                   42.13%  1.74342s         3  581.14ms  19.768us  1.74336s  cudaMallocManaged
                    0.56%  23.287ms         3  7.7625ms  7.0587ms  9.0555ms  cudaFree
                    0.02%  704.50us         1  704.50us  704.50us  704.50us  cuDeviceTotalMem
                    0.01%  216.34us        94  2.3010us     606ns  50.785us  cuDeviceGetAttribute
                    0.00%  76.115us         1  76.115us  76.115us  76.115us  cudaLaunch
                    0.00%  14.602us         1  14.602us  14.602us  14.602us  cuDeviceGetName
                    0.00%  10.685us         4  2.6710us     676ns  7.6680us  cudaSetupArgument
                    0.00%  4.5370us         1  4.5370us  4.5370us  4.5370us  cudaConfigureCall
                    0.00%  3.6480us         3  1.2160us     646ns  1.9600us  cuDeviceGetCount
                    0.00%  2.0180us         2  1.0090us     693ns  1.3250us  cuDeviceGet
                    0.00%     973ns         1     973ns     973ns     973ns  cudaGetLastError

==188== Unified Memory profiling result:
Device "Tesla V100-SXM2-16GB (0)"
   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name
    2304  170.67KB  4.0000KB  0.9961MB  384.0000MB  42.44723ms  Host To Device
     768  170.67KB  4.0000KB  0.9961MB  128.0000MB  11.30275ms  Device To Host
     768         -         -         -           -  110.5986ms  Gpu page fault groups
Total CPU Page faults: 1536
```

![1584244854962](https://i.loli.net/2020/04/06/2oLcISneMBVjgTi.png)

```c
#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include "timer.h"
#include "check.h"

#define SOFTENING 1e-9f

/*
 * Each body contains x, y, and z coordinate positions,
 * as well as velocities in the x, y, and z directions.
 */

typedef struct { float x, y, z, vx, vy, vz; } Body;

/*
 * Do not modify this function. A constraint of this exercise is
 * that it remain a host function.
 */

void randomizeBodies(float *data, int n) {
  for (int i = 0; i < n; i++) {
    data[i] = 2.0f * (rand() / (float)RAND_MAX) - 1.0f;
  }
}

/*
 * This function calculates the gravitational impact of all bodies in the system
 * on all others, but does not update their positions.
 */

__global__
void bodyForce(Body *p, float dt, int n) {
  int index = threadIdx.x + blockIdx.x * blockDim.x;
  int stride = blockDim.x * gridDim.x;

  for (int i = index; i < n; i += stride) {
    float Fx = 0.0f; float Fy = 0.0f; float Fz = 0.0f;
    for (int j = 0; j < n; j++) {
      float dx = p[j].x - p[i].x;
      float dy = p[j].y - p[i].y;
      float dz = p[j].z - p[i].z;
      float distSqr = dx*dx + dy*dy + dz*dz + SOFTENING;
      float invDist = rsqrtf(distSqr);
      float invDist3 = invDist * invDist * invDist;

      Fx += dx * invDist3; Fy += dy * invDist3; Fz += dz * invDist3;
    }

    p[i].vx += dt*Fx; 
    p[i].vy += dt*Fy; 
    p[i].vz += dt*Fz; 
  }

}
__global__ void add(Body*p, float dt,int n) {
  int index = threadIdx.x + blockIdx.x * blockDim.x;
  int stride = blockDim.x * gridDim.x;
  for (int i = index; i < n; i += stride) {
    p[i].x += p[i].vx*dt;
    p[i].y += p[i].vy*dt;
    p[i].z += p[i].vz*dt;
  }
}

int main(const int argc, const char** argv) {
  int deviceId;
  int numberOfSMs;

  cudaGetDevice(&deviceId);
  cudaDeviceGetAttribute(&numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId);

  /*
   * Do not change the value for `nBodies` here. If you would like to modify it,
   * pass values into the command line.
   */

  int nBodies = 2<<11;
  int salt = 0;
  if (argc > 1) nBodies = 2<<atoi(argv[1]);

  /*
   * This salt is for assessment reasons. Tampering with it will result in automatic failure.
   */

  if (argc > 2) salt = atoi(argv[2]);

  const float dt = 0.01f; // time step
  const int nIters = 10;  // simulation iterations

  int bytes = nBodies * sizeof(Body);
  float *buf;

  cudaMallocManaged(&buf, bytes);
  //cudaMemPrefetchAsync(buf, bytes, deviceId);

  Body *p = (Body*)buf;

  /*
   * As a constraint of this exercise, `randomizeBodies` must remain a host function.
   */

  randomizeBodies(buf, 6 * nBodies); // Init pos / vel data

  size_t threadsPerBlock = 256;
  size_t numberOfBlocks = 32 * numberOfSMs;

  double totalTime = 0.0;

  /*
   * This simulation will run for 10 cycles of time, calculating gravitational
   * interaction amongst bodies, and adjusting their positions to reflect.
   */

  /*******************************************************************/
  // Do not modify these 2 lines of code.
  for (int iter = 0; iter < nIters; iter++) {
    StartTimer();
  /*******************************************************************/

  /*
   * You will likely wish to refactor the work being done in `bodyForce`,
   * as well as the work to integrate the positions.
   */
  bodyForce<<< numberOfBlocks, threadsPerBlock >>>(p, dt, nBodies); // compute interbody forces  
  cudaDeviceSynchronize();
  add<<< numberOfBlocks, threadsPerBlock >>>(p, dt, nBodies);
  // Do not modify the code in this section.
    const double tElapsed = GetTimer() / 1000.0;
    totalTime += tElapsed;
  }
  cudaDeviceSynchronize();

  double avgTime = totalTime / (double)(nIters);
  float billionsOfOpsPerSecond = 1e-9 * nBodies * nBodies / avgTime;

#ifdef ASSESS
  checkPerformance(buf, billionsOfOpsPerSecond, salt);
#else
  checkAccuracy(buf, nBodies);
  printf("%d Bodies: average %0.3f Billion Interactions / second\n", nBodies, billionsOfOpsPerSecond);
  salt += 1;
#endif
  /*******************************************************************/

  /*
   * Feel free to modify code below.
   */

  cudaFree(buf);
}

```

复制工作目录（上文高亮部分），并替换以下代码块中的 ##FIXME##。完成替换之后，执行此单元 (Shift+Enter) 以将其存储到变量 MODEL_JOB_DIR 中

```c
MODEL_JOB_DIR = '/dli/data/digits/20180301-185638-e918'  ## Remember to set this to be the job directory for your model
!ls $MODEL_JOB_DIR
```

```
caffe_output.log   snapshot_iter_735.caffemodel   status.pickle
deploy.prototxt    snapshot_iter_735.solverstate  train_val.prototxt
original.prototxt  solver.prototxt
```



复制并粘贴成功后，您将看到该目录中所有文件的列表。如果以下说明与您所见不符，请检查复制/粘贴操作是否成功。

我们的“模型”同样包含两个文件：网络结构和权重。

网络结构文件名为 `deploy.prototxt`，权重则在最新生成的快照文件中（文件名为 `snapshot_iter_#.caffemodel.`）在本例中，快照编号 735 包含 5 次训练全部完成后习得的权重。

```c
ARCHITECTURE = MODEL_JOB_DIR + '/' + 'deploy.prototxt'
WEIGHTS = MODEL_JOB_DIR + '/' + 'snapshot_iter_735.caffemodel'
print ("Filepath to Architecture = " + ARCHITECTURE)
print("Filepath to weights = "+ WEIGHTS)
```

```c
Filepath to Architecture = /dli/data/digits/20180301-185638-e918/deploy.prototxt
Filepath to weights = /dli/data/digits/20180301-185638-e918/snapshot_iter_735.caffemodel
```



接下来，我们需要确保正在构建的程序可以读取并处理这些文件。在部署中，我们需安装模型训练时所用的深度学习框架，以便能够解析它们。稍后，我们会在本课程中学习如何将模型部署至无需安装框架的环境中。我们还需使用 GPU的并行处理能力。我们的模型包含数十万次操作运算，可通过并行计算予以显著加速。

```c
import caffe
caffe.set_mode_gpu()
```

接下来，我们将创建名为“net”的“分类器”对象。工作流程越常见，您就越容易利用现有工具创建项目。在本例中，图像分类十分常见，因此下一个代码块只需使用网络结构文件和权重文件以及部分数据，简化了常见操作。

```c
# Initialize the Caffe model using the model trained in DIGITS
net = caffe.Classifier(ARCHITECTURE, WEIGHTS,  
                       channel_swap =(2, 1, 0), #Color images have three channels, Red, Green, and Blue.
                       raw_scale=255) #Each pixel value is a number between 0 and 255
                       #Each "channel" of our images are 256 x 256 
```

“分类器”这个类包括一种“预测”方法，该方法会提取上文所定义图像的输入，输出该图像属于每一类别的概率。

创建预期输入：预处理)

我们首先来进行一项简单的任务：尝试从数据集中正确分类已标记图像。我们可以通过运行下方单元来加载图像并进行查看。

```
import matplotlib.pyplot as plt #matplotlib.pyplot allows us to visualize results
input_image= caffe.io.load_image('/dli/data/dogscats/train/cats/cat.10941.jpg')
plt.imshow(input_image)
plt.show()
```

虽然我们已获得此图像，但它并非网络所期望的“输入”。

在准备推理数据时，我们需遵循一条黄金法则：

> 训练前的所有操作均需在推理之前完成

在上一节中，您看到了 DIGITS 在训练模型时生成的文件。而在本节，我们将检测 DIGITS 在创建数据集时生成的文件。

当您从模型页面“狗和猫”中选择数据集及/或从 DIGITS“数据集”选项卡下选择数据集时，便可找到所训练**数据集**的工作目录。工作目录与模型所在位置相同，只不过编号应会有所不同。

使用此工作目录替换 ##FIXME##，并执行下方代码以将 DATA_JOB_DIR 设置为正确的文件路径，然后检查此路径下的内容：

```
DATA_JOB_DIR = '/dli/data/digits/20180222-165843-ada0'  ## Remember to set this to be the job directory for your model
!ls $DATA_JOB_DIR
```

```
create_train_db.log  labels.txt        mean.jpg       train.txt  val.txt
create_val_db.log    mean.binaryproto  status.pickle  train_db	 val_db
```



同样，此处会提供超过您目前所需的更多信息。您*能够*了解有关数据科学和数据准备的海量信息，并会在解决各类深度学习问题后获得更清晰的认知。在本例中，DIGITS 在训练之前执行了两个步骤。我们称之为*预处理*。

1) DIGITS 将图像调整为 256X256 大小的彩色图像

```python
import cv2
input_image=cv2.resize(input_image, (256, 256), 0,0)
plt.imshow(input_image)
plt.show()
```

2) DIGITS 通过从每张图像中减除均值图像以将图像*标准化*，以此减少训练所需的计算量。

加载均值图像，并将其从以下测试图像中减除：

```python
mean_image = caffe.io.load_image(DATA_JOB_DIR+'/mean.jpg')
ready_image = input_image-mean_image
```

我们现已取得原始数据，并已将其转换为网络所期望的数据。接下来，我们来看一下网络所创建的输出。

## 前向传播：使用您的模型

这就是我们的关注所在。我们来看看这个函数： `prediction = net.predict([grid_square])`.

与其他任一 [函数](https://www.khanacademy.org/computing/computer-programming/programming#functions) 类似，`net.predict` 也会传递输入 `ready_image`，并返回输出 `prediction`。但与其他函数不同，该函数并不遵循一组步骤，而是逐层开展矩阵数学运算，以将图像转换为概率向量。

运行下方单元，以查看对以上已标记数据作出的预测。

```python
# make prediction
prediction = net.predict([ready_image])
print prediction
--[[ 0.70993775  0.29006225]]
```

结果很有趣，但并未包含太多信息。我们的网络获取到 256x256 大小的标准化彩色图像，并生成一个长度为 2 的向量。

## 生成有用的输出：后处理

此时，我们便可真正构建想要的任何内容。而唯一的局限在于您的编程经验。在进行创意操作之前，我们先来构建一些基本内容。此代码将能确定网络输出“狗”的概率是否高于“猫”。若为此，当狗接近模拟的狗门时，其将显示适当的图像。否则，若网络确定门前是只猫，图像便会呈现我们希望看到的内容。

```python
print("Input image:")
plt.imshow(input_image)
plt.show()

print("Output:")
if prediction.argmax()==0:
    print "Sorry cat:( https://media.giphy.com/media/jb8aFEQk3tADS/giphy.gif"
else:
    print "Welcome dog! https://www.flickr.com/photos/aidras/5379402670"
```

![1584367324218](https://i.loli.net/2020/03/29/jewqLO3yNzpb8Fm.png)

此时，我们已在此处做好所有准备工作，您现在可以对狗门可能会看到的图像进行测试。

```
##Create an input our network expects
input_image= caffe.io.load_image('/dli/data/fromnest.PNG')
input_image=cv2.resize(input_image, (256, 256), 0,0)
ready_image = input_image-mean_image
##Treat our network as a function that takes an input and generates an output
prediction = net.predict([ready_image])
print("Input Image:")
plt.imshow(input_image)
plt.show()
print(prediction)
##Create a useful output
print("Output:")
if prediction.argmax()==0:
    print "Sorry cat:( https://media.giphy.com/media/jb8aFEQk3tADS/giphy.gif"
else:
    print "Welcome dog! https://www.flickr.com/photos/aidras/5379402670"
```

![image-20200406121449834](https://i.loli.net/2020/04/06/MWaQ4FCjhv91HiS.png)

基本上，我们已经为狗门挑战创建出模拟器。我们已创建一款应用程序，其能从摄像机中提取输入、将其转换为网络所期望的数据类型、生成输出，并将该输出转换为对用户有用的内容。

您能够了解如何通过正确的输出来轻松控制狗门的动力开关装置。在深度学习方面，您已掌握所需的一切！如要查看可在以上代码块中试用的其他图像`列表`，您可通过运行下方命令来找到测试图像（不用于训练的图像）。其中一些图像预计会输出错误分类。您需要不断测试这些图像直到取得满意的结果为止，然后再继续课程学习，以了解如何提高性能！

## 融会贯通

我们来系统全面地理解一下该部署过程，看看它在此 Jupyter notebook 之外会是什么样。在 [pythondeployment.py](http://ec2-54-227-229-252.compute-1.amazonaws.com/Bamaudsn/edit/tasks/task3/task/pythondeployment.py) 的 Python 文件中，您将看到与上文相同的代码，不过此处代码已合并至同一个文件。在课程末尾的“评估”一节中，您将使用此方法，所以现在请作一些了解。将文件路径插入到此处的测试图像，以对其进行可视化。

```python
TEST_IMAGE = '/dli/data/dogscats/test/1.jpg'
display= caffe.io.load_image(TEST_IMAGE)
plt.imshow(display)
plt.show()
```

然后，运行我们的小型 python 应用程序，并按下方输入该图像。请忽略输出的大部分内容，并滚动至底部。（即使出现错误和警告也没关系。）

```python
!python pythondeployment.py $TEST_IMAGE 2>/dev/null
[[ 0.42844081  0.57155913]]
Output:
Welcome dog! https://www.flickr.com/photos/aidras/5379402670
None
```



# 目标检测

### 使用部署

滑动窗口法需要部署经过训练的图像分类器，以在应用程序中将图像的 256X256 图块分类为“狗”或“猫”，而应用程序则会每次以 256X256 大小的窗口在图像上移动。虽然这一解决方案并不完美，但却可以：

1) 构建一个综合运用深度学习和传统编程的典型方法，从而打造出先前无法构建的应用程序 2) 引导启发学习神经网络架构

在本课程第一节中，您已学习如何成功*训练*及*部署*神经网络。您了解到，传统编程不足以支持图像分类，而深度学习不仅能够实现图像分类，还能以直观方式进行呈现。

到目前为止，您已采用现有神经网络和开源数据集（狗和猫），并创建出一个模型，用来对与初始数据集相像及不相像的新动物进行正确分类。这相当厉害！

在**此**任务中，您将学习如何通过以下方式以利用深度学习解决*图像分类之外*的问题：

- 将深度学习与传统计算机视觉相结合
- 修改神经网络的内部结构（实际数学运算的部分）
- 选择适合任务的神经网络

并进而解决第二项深度学习挑战：

**我们能否在图像中检测并定位物体？**

首先，我们会采用与上一节相同的方式，将模型实例化。我们引入模型和数据集工作目录来利用模型架构、经过训练的权重和预处理信息（如均值图像）。

```python
import time
import numpy as np #Data is often stored as "Numpy Arrays"
import matplotlib.pyplot as plt #matplotlib.pyplot allows us to visualize results
import caffe #caffe is our deep learning framework, we'll learn a lot more about this later in this task.
%matplotlib inline

MODEL_JOB_DIR = '/dli/data/digits/20180301-185638-e918'  ## Remember to set this to be the job directory for your model
DATASET_JOB_DIR = '/dli/data/digits/20180222-165843-ada0'  ## Remember to set this to be the job directory for your dataset

MODEL_FILE = MODEL_JOB_DIR + '/deploy.prototxt'                 # This file contains the description of the network architecture
PRETRAINED = MODEL_JOB_DIR + '/snapshot_iter_735.caffemodel'    # This file contains the *weights* that were "learned" during training
MEAN_IMAGE = DATASET_JOB_DIR + '/mean.jpg'                      # This file contains the mean image of the entire dataset. Used to preprocess the data.

# Tell Caffe to use the GPU so it can take advantage of parallel processing. 
# If you have a few hours, you're welcome to change gpu to cpu and see how much time it takes to deploy models in series. 
caffe.set_mode_gpu()
# Initialize the Caffe model using the model trained in DIGITS
net = caffe.Classifier(MODEL_FILE, PRETRAINED,
                       channel_swap=(2,1,0),
                       raw_scale=255,
                       image_dims=(256, 256))

# load the mean image from the file
mean_image = caffe.io.load_image(MEAN_IMAGE)
print("Ready to predict.")
```

接着，我们将直接从前门监控摄像头中获取一张图像。请注意，此图像大于 256X256。我们想知道狗在此图像中位于*何处*。

```python
# Choose a random image to test against
#RANDOM_IMAGE = str(np.random.randint(10))
IMAGE_FILE = '/dli/tasks/task5/task/images/LouieReady.png'
input_image= caffe.io.load_image(IMAGE_FILE)
plt.imshow(input_image)
plt.show()
```

### 使用我们所学的函数：前向传播

这就是我们的关注所在。我们来看看这个函数： `prediction = net.predict([grid_square])`。

与其他任一 [函数](https://www.khanacademy.org/computing/computer-programming/programming#functions)类似，`net.predict` 也会传递输入 `grid_square`，并返回输出 `预测`。但与其他函数不同，该函数并不遵循一组步骤，而是逐层开展矩阵数学运算，以将图像转换为概率向量。

运行以下单元，以从我们随机图像左上方的 256X256 `grid_square` 查看预测。

```python
X = 0
Y = 0

grid_square = input_image[X*256:(X+1)*256,Y*256:(Y+1)*256]
# subtract the mean image (because we subtracted it while we trained the network - more on this in next lab)
grid_square -= mean_image
# make prediction
prediction = net.predict([grid_square])
print prediction
```

这是所用网络最后一层的输出。层类型为“softmax”，其中每个单元的值越高，图像属于该类的可能性就越大。在上例中，如果向量的第二个单元高于第一个（且网络已经过训练），则该部分图像便可能包含*狗\*。由于在本例中，第一个单元*远*高于第二个，因此很显然，网络并未在左上方的 256x256 网格中检测到狗。

可选：[探索 softmax 的数学运算。](https://en.wikipedia.org/wiki/Softmax_function)

接下来，我们将对此函数实施一些代码以对图像进行迭代，并对每个 grid_square 进行分类以创建热图。请注意，本节的课程关键在于，我们可以在此函数上构建“任何东西”，但凡所想，均可构建。

根据具体编程（尤其是 Python）操作的差异，您或许会从以下单元获得截然不同的信息。其核心是，这个块会把我们的输入图像分割成 256X256 大小的方块，并会在我们的狗分类器中对其逐一运行；之后此块会根据分类器所得结果创建新图像，其中没有狗时图像为蓝色，而有狗时则为红色。

```py
# Load the input image into a numpy array and display it
input_image = caffe.io.load_image(IMAGE_FILE)
plt.imshow(input_image)
plt.show()

# Calculate how many 256x256 grid squares are in the image
rows = input_image.shape[0]/256
cols = input_image.shape[1]/256

# Initialize an empty array for the detections
detections = np.zeros((rows,cols))

# Iterate over each grid square using the model to make a class prediction
start = time.time()
for i in range(0,rows):
    for j in range(0,cols):
        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]
        # subtract the mean image
        grid_square -= mean_image
        # make prediction
        prediction = net.predict([grid_square]) 
        detections[i,j] = prediction[0].argmax()
end = time.time()
        
# Display the predicted class for each grid square
plt.imshow(detections, interpolation=None)

# Display total time to perform inference
print 'Total inference time: ' + str(end-start) + ' seconds'
```

同样，这是随机选择的广域测试图像的输出结果和一个数组，显示输入到模型的每个非重叠 256x256 网格的预测类。

在我们更好地解决问题之前，请花一分钟时间回想刚才的操作，并回答以下问题：

**用您自己的话描述一下我们刚刚如何使用图像分类器检测物体？**

本节旨在说明任何深度学习解决方案都不过是从输入到输出的映射。这一实现手段使我们能够更易理解如何构建更复杂的解决方案。例如，Alexa、Google Assistant 和 Siri 之类的语音助手必须将原始声音数据转换为文本，并将文本转换为相应的理解，最后再将相应的理解转换为预期任务，例如播放您最喜爱的歌曲。



其复杂性影响之广堪比任何已经或能够借助计算机科学构建的内容。请思考，在解决此类编码/问题所面临的挑战中，有哪些因素是我们未考虑到的，又该如何对其进行处理？

可能的答案：[单击此处](http://ec2-54-173-199-14.compute-1.amazonaws.com/33avr1Ti/notebooks/tasks/task5/task/Object detection-zh.ipynb#answer1)

### 1.4 挑战性练习（可选）：



1.使网格大小保持在 256x256，修改代码以增加网格间的重叠，并获得更精细的分类地图。

2.修改代码以将多个网格批量传递到网络中进行预测。

答案：

```python
%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
import caffe
import time

MODEL_JOB_NUM = '##FIXME##'  ## Remember to set this to be the job number for your model
DATASET_JOB_NUM = '##FIXME##'  ## Remember to set this to be the job number for your dataset

MODEL_FILE = '/dli/data/digits/' + MODEL_JOB_NUM + '/deploy.prototxt'                 # Do not change
PRETRAINED = '/dli/data/digits/' + MODEL_JOB_NUM + '/snapshot_iter_735.caffemodel'    # Do not change
MEAN_IMAGE = '/dli/data/digits/' + DATASET_JOB_NUM + '/mean.jpg'                      # Do not change

# load the mean image
mean_image = caffe.io.load_image(MEAN_IMAGE)

# Choose a random image to test against
#RANDOM_IMAGE = str(np.random.randint(10))
IMAGE_FILE = '/dli/data/LouieReady.png' 

# Tell Caffe to use the GPU
caffe.set_mode_gpu()
# Initialize the Caffe model using the model trained in DIGITS
net = caffe.Classifier(MODEL_FILE, PRETRAINED,
                       channel_swap=(2,1,0),
                       raw_scale=255,
                       image_dims=(256, 256))

# Load the input image into a numpy array and display it
input_image = caffe.io.load_image(IMAGE_FILE)
plt.imshow(input_image)
plt.show()

# Calculate how many 256x256 grid squares are in the image
rows = input_image.shape[0]/256
cols = input_image.shape[1]/256

# Subtract the mean image
for i in range(0,rows):
    for j in range(0,cols):
        input_image[i*256:(i+1)*256,j*256:(j+1)*256] -= mean_image
        
# Initialize an empty array for the detections
detections = np.zeros((rows,cols))
        
# Iterate over each grid square using the model to make a class prediction
start = time.time()
for i in range(0,rows):
    for j in range(0,cols):
        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]
        # make prediction
        prediction = net.predict([grid_square])
        detections[i,j] = prediction[0].argmax()
end = time.time()
        
# Display the predicted class for each grid square
plt.imshow(detections)
plt.show()

# Display total time to perform inference
print 'Total inference time (sliding window without overlap): ' + str(end-start) + ' seconds'

# define the amount of overlap between grid cells
OVERLAP = 0.25
grid_rows = int((rows-1)/(1-OVERLAP))+1
grid_cols = int((cols-1)/(1-OVERLAP))+1

print "Image has %d*%d blocks of 256 pixels" % (rows, cols)
print "With overlap=%f grid_size=%d*%d" % (OVERLAP, grid_rows, grid_cols)

# Initialize an empty array for the detections
detections = np.zeros((grid_rows,grid_cols))

# Iterate over each grid square using the model to make a class prediction
start = time.time()
for i in range(0,grid_rows):
    for j in range(0,grid_cols):
        start_col = int(j*256*(1-OVERLAP))
        start_row = int(i*256*(1-OVERLAP))
        grid_square = input_image[start_row:start_row+256, start_col:start_col+256]
        # make prediction
        prediction = net.predict([grid_square])
        detections[i,j] = prediction[0].argmax()
end = time.time()
        
# Display the predicted class for each grid square
plt.imshow(detections)
plt.show()

# Display total time to perform inference
print ('Total inference time (sliding window with %f%% overlap: ' % (OVERLAP*100)) + str(end-start) + ' seconds'

# now with batched inference (one column at a time)
# we are not using a caffe.Classifier here so we need to do the pre-processing
# manually. The model was trained on random crops (256*256->227*227) so we
# need to do the cropping below. Similarly, we need to convert images
# from Numpy's Height*Width*Channel (HWC) format to Channel*Height*Width (CHW) 
# Lastly, we need to swap channels from RGB to BGR
net = caffe.Net(MODEL_FILE, PRETRAINED, caffe.TEST)
start = time.time()
net.blobs['data'].reshape(*[grid_cols, 3, 227, 227])

# Initialize an empty array for the detections
detections = np.zeros((rows,cols))

for i in range(0,rows):
    for j in range(0,cols):
        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]
        # add to batch
        grid_square = grid_square[14:241,14:241] # 227*227 center crop        
        image = np.copy(grid_square.transpose(2,0,1)) # transpose from HWC to CHW
        image = image * 255 # rescale
        image = image[(2,1,0), :, :] # swap channels
        net.blobs['data'].data[j] = image
    # make prediction
    output = net.forward()[net.outputs[-1]]
    for j in range(0,cols):
        detections[i,j] = output[j].argmax()
end = time.time()
        
# Display the predicted class for each grid square
plt.imshow(detections)
plt.show()

# Display total time to perform inference
print 'Total inference time (batched inference): ' + str(end-start) + ' seconds'
```

正如当前所示，此滑动窗口法的优点在于我们可以仅使用基于局部的训练数据（可以更广泛地获取）训练检测器，且能帮助我们凭借当前的技能组合着手解决问题。

我们可以不断调整代码以确认自己能否继续通过蛮力解决问题（错误且缓慢），但也可学习更多有关深度学习的知识。我们来选择第二种方式。

## 方法 2 – 基于已有神经网络进行重建[¶](http://ec2-54-173-199-14.compute-1.amazonaws.com/33avr1Ti/notebooks/tasks/task5/task/Object detection-zh.ipynb#方法-2-–-基于已有神经网络进行重建)

请记住，深度神经网络的构建灵感源自于人脑。本节将说明如何*更改*网络以改变其行为和性能。就其核心而言，这些网络均由数学运算组成，改变数学运算即会改变大脑。本节不会全面深入地解释各类网络可完美解决各类问题的具体*原因*，因为这是一种会随时间逐渐累积的直觉知识。相反，本节将侧重介绍您在试验期间需牢记的限制条件。我们可能会介绍一些常见的层类型，但这并非本课程的内容要点。

我们先来探讨一下当前的网络 AlexNet 吧。

### 2.1 当前网络

这些网络的结构均会在某类*框架*中进行说明，而本例中使用的是 Caffe 框架。[框架种类数不胜数](https://developer.nvidia.com/deep-learning-frameworks)，其中每种框架都能使从业者对网络进行宏观描述，而不必对 GPU 进行物理编程以执行张量数学运算。





















## 方法 3：DetectNet

在此实验开始时，我们采用的是 AlexNet，这是一款针对极具体*图像分类*问题的卓越解决方案。我们对其构建一些 Python 代码，然后开展轻微的网络手术，以此来完成一项在利用深度学习开始*目标检测*工作之前本不可能达成的任务。

不过，是否存在一款专门针对*目标检测*的卓越解决方案？我们能否建立一个模型，使其直接从输入（航拍照片）映射至预想的输出（定位和检测信息）？

此解决方案的存在将有助我们拓广对深度学习的定义，并能帮助您深入了解可借助其解决哪些*其他*问题。

本课程首先讨论的是深度学习得以实现的三个要素。

1) 深度神经网络 2) GPU 3) 大数据

当我们超出图像分类的讨论范围时，以上要素仍保持不变，但网络类型、数据和 GPU 的使用会有所不同。我们将从数据谈起，因为这是我们的工作流程在 DIGITS 中的组织方式。

### 数据差异

如要构建端到端监督式深度学习工作流程，我们需要已标记的数据。到目前为止，我们已将“已标记”定义为每个图像所属类别的指标。然而，标记数据的真正目的在于**创建输入输出映射。”

在本例中，我们希望输入“任意”尺寸的图像，且输出结果应能表明物体在图像中的所处位置。首先，请输入图像：

```py
input_image = caffe.io.load_image('/dli/data/train/images/488156.jpg') #!ls this directory to see what other images and labels you could test
plt.imshow(input_image)
plt.show()
```

然后，为其加上对应标签：

```py
!cat '/dli/data/train/labels/488156.txt' #"cat" has nothing to do with the animals, this displays the text
```

注意：

1) 输入及其对应输出基于文件编号一一照应。 2) 所见向量由输入图像中狗的左上方和底部位置的 (x,y) 坐标构成。 3) 如果有足够的冲浪板数据，我们则可训练冲浪板检测器而非狗检测器！在本例中，我们会将数据集设置为只寻找狗。

将此数据集加载到 DIGITS 中。

从 [DIGITS](http://ec2-54-173-199-14.compute-1.amazonaws.com/digits) 主页面中，选择“Datasets”（数据集），然后添加新的**“目标检测”数据集。**请注意，这些参数看起来不同于进行分类时的参数。

打开“New Object Detection Dataset”（新的目标检测数据集）面板后，请使用以下图像的预处理选项（**请留意前部/尾部间距**）：

训练图像文件夹：/dli/data/train/images 训练标签文件夹：/dli/data/train/labels 验证图像文件夹：/dli/data/val/images 验证标签文件夹：/dli/data/val/labels/dog 补齐图像（宽 x 高）：640 x 640 自定义类：dontcare, dog 组名称：MS-COCO 数据集名称：coco-dog