---
title:  "Linux下关于软链接和硬链接，动态链接库"
date:   2019-09-07 10:16:18 +0800
categories:
- C++
tags:
- C++ 
---



这篇文章主要介绍了Linux内核的一些笔记等知识。

<!--more-->

# Linux内核（李林）

1.在内核里打印就不能用`printf`函数了，那是用户态用的，现在在内核里要用`printk`函数。

2.在内核里查看调试信息使用`dmesg`命令。嫌太长可以配合`dmesg | tail`命令查看尾部。

3.`make出来*.ko`文件之后，使用`insmod *.ko`进行安装内核文件（卸载使用`rmmod` 命令），这时initialize函数（使用`module_init(xxxxInitialize)`宏来注册该函数）会被首先执行，类似于我们在单片机里写的那些初始化寄存器，配置`IO`管脚的那些功能。

## Makefile文件编写

> obj-m说明有一个模块需要从目标文件PrintModule.o中构造，而该模块名为PrintModule.ko
>
> 说明PrintModule由多个目标文件构成；一个编译单元一个目标文件（.o文件）
>
> -DTEST_DEBUG：自定义宏
>
> -ggdb：加入调试信息
>
> -O0：优化级别

![1583549085179](https://i.loli.net/2020/03/29/BuXplzwcrVi6FZa.png)

![1583549416625](https://i.loli.net/2020/03/29/9kUaThAX6MDjgr5.png)

![1583549604315](https://i.loli.net/2020/03/29/qHGhglabk5M8SRr.png)

顶层的系统内核中的`makefile`里面定义了`kernalrelease`，编译时需要调用顶层的这个文件所以需要`kernaldir`

## 优化与调试级别

> n-O0：没有优化，默认选项
>
> n-O1：基本优化级别
>
> n-O2：主要是优化时间效率，不考虑生成的目标文件大小
>
> n-O3：最高优化级别
>
> n-Os：优化生成的目标文件大小，并且激活-O2中的不增加代码大小的优化选项
>
> n-Og：gcc 4.8中引入的优化级别。针对快速编译和超强的调试体验，并同时提供合理的运行效率。该级别比使用-O0整体效果好。

![1583550194820](https://i.loli.net/2020/03/29/SRwkiTvWQat7oXA.png)

## 内核开发特点

![1583551474801](https://i.loli.net/2020/03/29/NuHSivUVKxIPB2o.png)

![1583551441783](https://i.loli.net/2020/03/29/6TUeK3xdYBztwbV.png)

大概率会发生的汇编代码会放在条件跳转的紧接着的地址，这样CPU一取多了就会把大概率发生的指令一块就给取了，而低概率发生的代码则放在更高的地址上也就是远离xx条件跳转的地方，因为它不太可能发生的嘛所以CPU一次也不太可能把那么远的指令给取过来。

![1583551273362](https://i.loli.net/2020/03/29/z2nGZOCyJ8FSHxu.png)

## 编译内核

1.首先查看一下本系统使用的内核版本号：虚拟机输入命令 `uname `

`Linux ubuntu 5.0.0-23-generic #24~18.04.1-Ubuntu SMP Mon Jul 29 16:12:28 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux`

2.然后使用apt搜一下，看看有没有对应的最新版：

`apt search linux-source`

3.然后，安装即可，安装后到/usr/src目录查找。

`apt install linux-source-4.4.0`

4.进入`/usr/src/linux-source-4.4.0`目录后，解压到自己的home目录（注意：压缩包自建好了文件夹，不用建新的了）：

`tar xjvf linux-source-4.4.0.tar.bz2 -C ~`

![image-20200331181547030](https://i.loli.net/2020/03/31/7ead8bsH4PuXM5t.png)

![image-20200331181636905](https://i.loli.net/2020/03/31/v6Jc8yiDFSHu1fr.png)

![image-20200331181622179](https://i.loli.net/2020/03/31/JYbkUOyqCTLlWjm.png)

# 一个古老又广泛的寻址技术：段寄存器

### **早期用途：内存管理**

段寄存器是一个已经或者说即将被淘汰的CPU寄存器设计。诞生于16位CPU，通过使用段寄存器，16位的地址总线CPU就可以使用32位地址访问到4GB的大内存。

但是到了x86时代，地址总线变成了32位，段寄存器的最大作用消失。

这个时候，一个合理的思路是，为何不仿照16位的段寄存器访问32位地址空间那样，使用32位的段寄存器访问大于4GB的内存空间？

后来因为有了一个竞争技术--page的页管理方式的出现。

![1584085826895](https://i.loli.net/2020/03/29/XDs2H4oxfrglPCv.png)

在Linux下将4GB的内存空间划分为了三个区域（zone），其中的high-mem，也就是高端的896MB内存不是像普通内存normal zone一样，给应用程序直接长期映射来用。Highmem是专门用于访问超过4GB的内存的。

**因为在Intel系列的CPU设计中，寻址即使到了64位也不是用64位的地址直接放到地址总线上去索引，而是通过查多级索引表来确定最后的地址**（由于段寄存器只有16位，而现在64位也只使用了48位，所以能效上一致）。多级索引表在内存中，每一级都有地址存在内存中，这个存储就可以任意的长度。显然，这个多级索引需要CPU支持（不然32位的地址空间永远只能寻址到32位的地址空间）。Highmem在x86下就是与硬件的pae技术配合才能工作的。但是highmem是一个操作系统层面的技术，他的目标就是可以把超过4GB的物理内存动态的映射到4GB的内存空间。所以如果CPU不是Intel，pae技术就需要其他的CPU的硬件支持。例如ARM的LPAE。

这种扩张x86的可用内存的目标，在Windows下除了PAE机制外，还有4GT，用进程可见的线性内存空间从2GB扩张到3GB（与Linux一样），这个并没有本质的增大物理内存空间，只是扩大了进程可用的线性地址空间。还有AWE，可以让进程直接申请大端内存映射到自己的线性地址空间中。所以AWE要使用硬件的PAE机制才能使用超过4GB的内存。

既然使用大内存的技术在x86下已经有了，段寄存器在x86下就纯粹的变成了32位地址空间内的段地址安排，段地址（16位）：偏移（32位），就可以直接变成一个32位的绝对地址。必要性就不是很大了（虽然实模式还有要用，但是这个模式本身也是淘汰行列）。但是这个寄存器还是被强行用上了。Linux和Windows在x86下都有继续发掘这几个段寄存器的用法（虽然意义不大），创造一套新的内存管理机制（在x64下最终被淘汰）。

实模式是指段寄存器：寄存器的寻址方式，其中段寄存器中存放的实基地址，是一个地址偏移。保护模式也是段寄存器：寄存器的寻址模式，其中段寄存器中存放的是段选择子。也就是一个GDT/LDT序号。amd64架构下的模式叫做IA-32e，这个模式下有两个子模式：兼容模式和长模式。长模式直接不使用段寻址（FS/GS除外），兼容模式下仍然可以段寻址。

### **新的用途：系统调用**

段描述符是GDT和LDT。两级的段描述符表构成了段内存管理技术。这个技术的概念与ELF的程序段的概念遥相呼应，虽然并没有直接的关系，但是很容易想象ELF的段在内存中的映射和访问是可以用内核的段管理技术来访问的。但是实际上，段管理技术最终也只是使用的段地址（16位）：偏移（32位）访问的32位的地址空间。CPU的TLB和对应的地址索引技术提供了足够的针对地址的安全，权限等功能。与GDT和LDT表中的功能重叠。这就使得GDT和LDT表的存在很尴尬。内核在X86时代同时支持两种不同的内存访问方式。段式的访问和页式的访问。两种几乎为竞争关系。结果毋庸置疑，页式的完胜，到了x64，段寄存器就基本退出了历史舞台了。但是也只是又从通用的内存管理中退出，内存管理由page机制主要完成。而GDT和LDT控制的段内存访问机制，仍然被Linux带到了x64，作为页机制的补充机制。

但是即使在x64的长模式下，段寄存器依然被Windows和Linux广泛使用。这并不是Windows和Linux设计上的问题，而是Intel/AMD的设计问题。因为系统调用的逐渐的由int 80转向了sysenter/sysexit系列，后面又逐渐进入syscall的时代。而这个系列的系统调用指令，内部直接使用了段寄存器。段寻址就这样被固化下来了，在一个人们看不到的地方发挥着作用。

GDT和LDT这个二级表的概念并不是操作系统发明的，而是CPU发明的。有Lgdt这种对应的汇编指令和GDTR这种直接指定GDT位置的硬件寄存器。Linux下轻度依赖，基本上只用到了GDT。因为段寄存器的长度只有16位，所以最多一共只有65535字节的大小的GDT和LDT表（GDT相当于CPU给操作系统用的，LDT相当于给进程用的，每个进程一个，两者的结构和功能是一样的）。每一个entry占8个字节，所以一个表一共是最多8192个entry。在非64位长模式下，各种段索引都仍然可以使用，GDT，LDT仍然可以作为编程的技法。但是在ia-32e的长模式下，除了FS，GS之外，其他段寄存器都不使用GDT，LDT的段索引了（但是系统调用发生的时候修改的CS和SS是为的啥？），在ia-32e的兼容模式下，段索引仍然可以用。

但是实际上，由于Linux对段寻址的使用很轻量，LDT直接基本不使用，GDT里面的entry的数目也很有限。都是用于特殊目的的便捷寻址的（便捷吗？）。Windows下目前的win 10已经对用户彻底隐藏LDT。在Windows下有一个WOW64模式，是兼容运行32位程序的模式。在这个模式下，32位的程序可以运行在64位的CPU上，操作系统内核运行的是长模式，但是应用运行的是兼容模式（也就是ia-32e的两种子模式）。Windows通过控制在两种模式下切换，完成32位应用程序的系统请求（系统请求都是实现在64位的长模式下）。这个切换的过程实际上是CS值的变化，WOW64应用可以自己改变CS的值来达到在32位的程序中运行64位代码的目的。这个技术叫做地狱之门。wow64看到的fs,gs分别指向32位线程的TEB和64位线程的TEB，其他的四个段寄存器都指向一个相同的偏移。该偏移的作用未知。但是因为在兼容模式下，段寻址是有效的，所以可以很容易想到这个段选择子应该是4GB内存的根VAD，也就是4GB线性地址空间的基地址位置。

几个段寄存的名字就表明了CPU当时设计他们的初始目的。CS是代码段，DS是数据段，SS是堆栈段，ES是附加段。这个与ELF的段设计很相似，可惜ELF也没有用这套硬件机制来映射，毕竟ELF不止有这么四个段。

CPU硬件上设计了GDT和LDT，但是段寄存器就只有这四个（或算fs/gs的6个），所以在使用段寄存器的时候就得在寄存器里面指定访问的到底是GDT还是LDT了。所以16位的段寄存器里面并不都是段偏移。必经一个table最多有8192个entry，只需要13个位就可以完全索引。也正是因为如此，段寄存器就只使用了15-3这13个位来作为entry的索引。2位用于指定是GDT还是LDT，1，0两位就对应我们熟悉的4个权限级别ring0-ring3。这个是保护模式的定义。

所以就引出了段寄存器的另外一个功能：权限控制。页的分级访问也有一套完整的针对每个页的权限控制机制，与段entry里面的权限类似。但是16位的段寄存器里面竟然也有权限，并且只有两个位。这两个位就被Intel充分的挖掘了。虽然Intel没有明说，你会发现这两个位和ring0-ring3在实际的执行的时候总是对应的。当CS段寄存器的这两位指定了ring0，但是却是在ring3的模式下的代码，是不能执行的。SS寄存器也是一样。在sysenter之前，指向用户程序的栈，在进入内核后，由sysenter指向了该线程对应的内核栈。两个权限位就与CS是一样的与ring3和ring0保持一致。

Sysenter是一个从ring3切换到ring0的指令，它的工作原理依赖了几个专门设计的寄存器。IA32_SYSENTER_CS (0x174) 里面存放了系统调用所用到的CS，这个CS的最后两位的值就是00，也就是ring0的权限了。由于CS只有16位，而这个0x174寄存器却有32位。所以还有一些其他的信息存储。IA32_SYSENTER_ESP (0x175)内核用到的ESP，IA32_SYSENTER_EIP (0x176)就是内核的系统调用入口，当然是相对于CS的。

所以我们惊讶的发现，即使在64位下，所有的系统调用也是经过段寄存器的。段寄存器在系统中依然被重度的使用。但是，这么做究竟有没有必要？因为是硬件直接这么固化了，软件上就算认为没有必要也没有办法。他被用于系统调用是既定事实。但是在软件层面，段寄存器也确实从内存管理中逐渐的淡出。例如x86下，段寄存器在Windows上还可以用于DEP或者PEB，在x64长模式下，无论是Windows和Linux，都在软件层面弱化甚至消除段寄存器的依赖。硬件上也直接认为段选择子无效。

有一句话，很多时候只是听一听：所有的用户空间代码都是运行在ring3，所有的内核代码都是运行在ring 0。仔细想这句话就会发现很多不可思议的地方。这意味着，不论是root用户还是普通用户，都是无差别的受到这个限制。既然所有的内核代码都是在ring0中执行，那么就必然存在一个从用户空间到内核空间的ring切换的位置。这个位置在Linux内核代码里并没有找到。显然是一个硬件机制。这就是sysenter和CS段寄存器在起到的巨大作用。

### **一个特殊的用法：TLS**

在Windows下，情况更复杂。除了硬件sysenter使用的CS段寄存器。Windows自己更改了从用户到内核所使用的寄存器。线程运行在 RING0 下， FS 段值是 0x30 （ WindowsXP 下值，在 Windows2000 下值为 0x38 ）；运行在 RING3 下时， FS 段寄存器值是 0x3b 。 FS 寄存器值的改变是在程序从 Ring3 进入 Ring0 后和从 Ring0 退回到 Ring3 前完成的，也就是说：都是在 Ring0 下给 FS 赋不同值的。Windows使用FS来指向TEB（X86情况下，X64情况下GS指向TEB。可以通过检查GS是否为0来判断当前是否是纯32位系统，wow64下，FS和GS分别指向线程的32位和64位的TEB）。在ring3下，FS一直指向当前线程的TEB段。随着线程的切换就一直在切换。所以在用户空间的代码，可以放心的用FS来直接索引到TEB段的内存内容。在ring0下，Windows下的FS指向处理器控制区域（KPCR）对应的GDT段。这个区域中保存这处理器相关的一些重要数据值，如 GDT 、 IDT 表的值等等。

也就是说，Windows除了sysenter硬件使用的CS外，还额外改变使用了FS段寄存器。这个FS的使用可以用来实现TLS。在x86和x64下，windows的行为差不多。但是在Linux下区别就比较大。Linux的无论是x86还是x64，对TLS的支持都是在glibc中完成的。也就是说，虽然大家都要改CS，但是对于其他段寄存器的修改，Linux在glibc中，而Windows在内核里。并且Windows使用的是FS，glibc使用的是GS。

但是无论是GS还是FS，都是一个TLS的访问入口，指明TLS的在GDT中的位置（所有的段寄存器都是用来在GDT/LDT中充当索引的）。但是Linux下仍然需要提前设置这个位置，Windows下因为进入和设置都是在内核里，所以不需要。Linux下需要提前使用set_thread_area来将该一个线程的TLS地址设置到GDT中。因为线程上下文切换是发生在内核中，内核在上下文切换的时候就同时修改GDT中的这个线程对应的entry和GS寄存器。这样glibc中使用GS寄存器就可以直接索引到特定的GDT的entry，就可以找到对应的TLS地址了。内核保证了位于用户空间的glibc看到的GS寄存器都是指向存储TLS信息的GDT的entry（这里的指向值得是段选择子提供的GDT的index序号）。

从上述可以看到在Linux下的两点：1、TLS的数据的真实存储位置是glibc提供的，通过set_thread_area来告知内核。2、内核中存储线程TLS的位置是GDT表中的某一个entry，这个entry的index（也就是GS段寄存器在索引时需要使用的选择子），可以由用户提供，也可以由内核来选择一个可用的。Glibc下是由内核选择。

也就是说，TLS实际上还是存储在线程自己申请的内存空间中。内核只是帮忙在GDT中找到一个entry记录一下，在线程切换的时候帮用户空间设置一下这个段寄存器。从而，我们能发现，TLS的实现并不是必须要使用内核支持。可以简单的通过编译器和链接器的支持来做到。（定义线程变量的语句直接编译成一个类似.GOT的数据，lazy解析的时候再与真实的地址绑定解析）。所以TLS对于段寄存器来说，是弱需求，只是目前都在这么用。毕竟TLS是随着线程的增加而增加的，不使用内核支持实现的难度相对大一些。内核在GDT中找到的entry的index会通过set_thread_area的参数修改的方式告诉给用户。不同线程不一定一样，但是不同线程也可以共用一个entry。一个数量级是很多Linux内核里，这个可以选择的TLS entry只有三个。但是线程却有那么多个。

事实上，在Linux下，如果只是基于glibc的应用程序，所有的线程都是使用的GDT中三个entry的第一个。set_thread_area需要用户传一个user_desc结构体。

struct user_desc {

unsigned int entry_number;

unsigned long base_addr;

unsigned int limit;

unsigned int seg_32bit:1;

unsigned int contents:2;

unsigned int read_exec_only:1;

unsigned int limit_in_pages:1;

unsigned int seg_not_present:1;

unsigned int useable:1;

\#ifdef __x86_64__

unsigned int lm:1;

\#endif

};

在glibc下，用户传进来的entry_number是-1，意思是让内核选择index，然后通过修改这个值告诉用户结果。

Linux下几乎没有使用LDT，GDT中预留了三个给TLS用。Glibc让用户选择，最后选择的几乎一定是三个中的第一个。因为内核是遍历查找这三个，看有没有被使用。也就是说GS寄存器的值几乎是固定的。

这里有必要解释一下三个TLS的选择问题。因为GDT表是每个CPU核一个表，所以只需要考虑单个CPU是否会发生线程选择冲突。单个CPU在同一个时刻，物理上，只能有一个线程在运行。也就是说，这个线程永远都是独占这一个GDT的entry。内核线程上下文切换的时候会同时切换GDT中TLS的内容为线程的内容，并且设置GS寄存器。所以一般情况，我们在用户空间看到的GS的值都是一样的（0x33，也就是6号entry，取段寄存器的15-3位为index）。比如Wine会使用第二个TLS，就是用来模拟Windows下exe的执行的。Windows下也有类似的TLS机制，只不过是通过FS段寄存器实现，而不是GS。Windows在64下同时使用GS和FS来支持TLS，但是连读写fs,gs寄存器的权限都不提供了（FSGSBASE指令是一个ring3的指令扩展，可以用来读取FSGS的内容，需要CPU支持）。Linux在64位还提供了arch_prctl可以用来读写段寄存器。

Linux可以给每个进程（线程）都创建一个LDT，看起来这个LDT是一个可以充分利用的段描述符表。并且提供了modify_ldt可以操作这个表。但是这个在Linux下只是一个兼容机制，Linux不建议用这个LDT表来实现TLS，或者用于替他的目的。因为会降低线程切换的效率。只用于兼容16位的程序或者32位的段寻址程序。LDT基本被Linux废弃。实际上，LDT表，不显示的调用modify_ldt，Linux就不会生成。

### **段寻址的性能**

段寻址需要先查段表（GDT或者LDT），从段表中找到实际的目标地址内存块的初始位置，然后再跟便宜计算。由于段表是放在内存的，看起来每次都需要额外访问一次内存。实际上，CPU在设计的时候，实现了影子寄存器。影子寄存器相当于段表条目的缓存，使得段寄存器的段信息可以直接从寄存器中读取，从而达到与平台寻址（flat）相似的寻址性能。

### **总结**

段寄存器越来越对用户空间代码封闭，所以程序代码应该尽可能的少依赖使用段寄存器的trick。CS和SS被硬件sysenter/sysexit频繁使用（这个和paper中说的长模式不使用段寻址互相冲突，我不能解释）。FS/GS在64位的Windows下被操作系统完全控制。而64位会从硬件层面直接忽略段寄存器的寻址方式。也就是说，在64位下，段寄存器作为一种寻址方式是被禁止使用的，只能由sysenter/sysexit等硬件使用。段寄存器从一个功能巨大的内存管理方式，逐渐退步到内存管理的辅助，到TLS的最后阵地，最后逐渐变成了硬件专用的内部寄存器。发展的考虑，在任何情况下，应用程序继续显示的使用段寄存器都是不被鼓励的。

Trick技巧上，如果寄存器实在不够用了，可以考虑在linux下使用fs来寻址。

\#include <asm/prctl.h>

static int arch_prctl(int func, void *ptr) {

return syscall(__NR_arch_prctl, func, ptr);

}

arch_prctl(ARCH_SET_FS, (void*)fsbase);

mov rax,fs:[rcx+rdx*8]

### FS寄存器指向当前活动线程的TEB结构（线程结构）

偏移 说明
000 指向SEH链指针
004 线程堆栈顶部
008 线程堆栈底部
00C SubSystemTib
010 FiberData
014 ArbitraryUserPointer
018 FS段寄存器在内存中的镜像地址
020 进程PID
024 线程ID
02C 指向线程局部存储指针
030 PEB结构地址（进程结构）
034 上个错误号

### 



# 链接原理

[转载自--刘叔--深入Linux内核](https://zhuanlan.zhihu.com/p/52964760)

在编译的过程中，如果所有的代码都写到一个单独的文件里，由于编译器以文件为单位进行编译，所以可以一次性的拿到所有的函数，那么就可以就地处理所有的符号，显然这是编译器最喜闻乐见的事情。但是由于有外部库和工程组织的需要，不可能所有的代码都在同一个文件里，编译器是用来满足开发人员的需要的，不是反过来。所以编译器要想办法解决不同文件之间的链接问题。

编译器在编译一个文件的时候，会生成一个段的划分。这个划分通常名字大同小异（当然可以通过写link脚本改变段的命名和排布），但是.text, .data这种常用的代码段和数据段基本没有人会有其他想法。每个文件编译的时候生成了同样的.text段，链接器用来处理多个编译单元的（也就是.o文件），将这些文件链接在一起的时候，链接器的主要工作就是将同样的段进行合并。这个操作看起来简单，但是不断.o文件互相调用的情况该怎么解决呢？例如A文件调用了B文件的test函数，在编译A的时候看不到B中test函数的定义，那么这个A里面的B的test函数的地址该如何填充？链接器在进行链接的时候又该如何修正？

首先可以确定的是A里面在链接发生之前是肯定不能知道B中test的地址的，但是A里面的汇编结果的call指令的目的地址总需要填充个值。这个值就是0，就是在编译A的时候，发现A调用了别人的test函数，编译器会直接在call的A函数的位置填call 0地址，然后同时，在A的目标文件的一个.rel.text和.rel.data。这两个表叫做重定向表，用于在链接的时候组装不同的目标文件，一个是函数重定向，一个是数据重定向。里面存储的信息就是在A的某某偏移位置调用了test函数。当链接发生的时候，链接器查看A的重定向标发现A需要test的地址，然后在B的函数定义中查找test的定义和地址（是A和B的.text合并之后的地址），然后用这个地址去修改A对应的偏移里面的call指令。这样就完成了链接时的重定位。

这个重定位发生在所有的静态链接的时候，包括静态链接库的时候和链接自己的代码文件的时候。

但是我们知道还有一个很常见的应用是动态链接。动态链接的时候，符号的位置要在运行的过程中才会解析。编译的时候分为PIC的编译方式和非PIC的传统编译方式（现在大部分库都是使用PIC的方式）。两个的区别在于能不能在内存中重用库的代码。

非PIC的传统编译方式需要在加载库的时候就重新设置所有的符号。例如liba.so里面调用了libb.so里面的一个函数test，那么按照静态链接的思路liba.so需要暴露一个段里面存放需要重定位的符号（也就是 call test的偏移），在加载libb.so的时候就要立刻解析出liba.so里面的call指令对应的test函数的地址。

这种情况相当于liba.so的.text段的内容在加载的时候被修改了。也就是说liba.so的.text的位置在不同的程序里面是不一样的（因为libb.so在不同的进程不一定在同样的位置）。所以liba.so在内存中不能复用，也就是每个进程都要在内存中加载一份liba.so的.text，liba.so使用了多少次就需要加载多少份。

这样有问题吗？除了内存里有多份liba.so外，并没有什么问题。有一个特点是加载的时候需要解析全部的符号，即使没有用到的，这样加载的速度相对慢一些。

动态链接用到了.rel.dyn和.rel.plt（PLT：过程链接表）。前者是数据重定向，后者是函数重定向。两个段的功能与静态链接的重定向表是一样的。这一切都显得那么轻松。

但是毕竟程序员是追求完美的，针对这两个问题，追求完美的程序员想出了PIC模式。所谓的PIC模式就是位置无关，就是想办法让liba.so的.text在所有使用liba.so的进程之间复用。这样只依赖.rel.dyn和.rel.plt可能就做不到了。因为使用这两个表是需要修改.text段的内容的。所以又添加了.got和.got.plt两个表，同样的，前者对应数据，后者对应函数。这两个段就是PIC的实现方法了。

所谓的PIC就是在编译liba.so的call test函数的时候，不是在test函数的地址位置填充0，而是填充liba.so的.got.plt段的test地址。在编译的时候.got.plt中的test的地址是空的，显然是不能寻址的，但是call test指令却是直接固定的调用.got.plt的表的test函数的（虽然这个函数还不知道在哪定义）。.got.plt相当于一个桩子，call test就是调用了这里的桩子函数。由于.got.plt不是位于.text里面，所以在解析的时候只需要修改.got.plt里面的test的定义地址就可以找到真实的定义，无论libb.so加载到内存的什么位置，都是只需要找到它，然后填充liba.so的.got.plt的test函数条目即可。如此.text就可以实现复用了。第一个问题解决。

第二个问题就是延迟绑定的技术。这个技术是为了防止加载的时候解析所有的符号，而是让用的时候才解析。所需要的技术在应用了PIC之后几乎是现成的。就是.got.plt中的内容不是加载的时候填充，而是用到的时候填充。这一切由运行时的链接器完成（interpreter）。

## C编译与运行时栈

```c
int functt(int num) {
	num++;
	return num;
}
int main() {
	int ttt;
	ttt = functt(3);
    return 0;
}
```

![](https://i.loli.net/2019/09/07/JN12OYCeoGm4UXg.png)

？？一旦进入call，先将EBP压栈，因为接下来将把EBP移动到ESP位置，为什么要这么做？

call指令实际动作是push eip；mov eip，4111D6h连续两条指令.

ret指令实际动作是pop eip

我们总结一下：每次调用别的函数时，先将EIP指针入栈--然后将EBP入栈     

```c
case CAL:
	stack[top]=ip; 
	stack[top+1]=base;
	base=top; 
	break;
```

![图片参考](https://i.loli.net/2019/09/07/nk9YblSxXgqN4j2.png)

 [This is an reference-style link][1] 

？？为什么要将ESP往下移动192（0C0 Hex）个字节？

--他们说是为该函数留出临时存储区！192Bytes存储int数据（4Bytes）也只不过是48个int。当我们试图去分配一个50个元素的局部变量时看看会发生什么？

```c
int functt(int num) {
	int aaa[50] = { 0 };
	num++;
	return num;
}
```

![](https://i.loli.net/2019/09/07/3sbBi7eHh8Q4YlM.png)

ESP往低地址移动了190H=400Bytes，看来就是多留出一块局部变量的存储区域出来。

从lea指令到rep这条指令作用就是把为局部变量分配的内存空间填充CC数据。Stos将eax中数据放入es:[edi]中，同时edi增加4个字节。Rep使指令重复执行ecx次数。

[1]: https://www.cnblogs.com/mydomain/archive/2010/10/24/1860005.html





## 动态链接库



```c
/              根目录
├── bin     存放用户二进制文件
├── boot    存放内核引导配置文件
├── dev     存放设备文件
├── etc     存放系统配置文件
├── home    用户主目录
├── lib     动态共享库
├── lost+found  文件系统恢复时的恢复文件
├── media   可卸载存储介质挂载点
├── mnt     文件系统临时挂载点
├── opt     附加的应用程序包
├── proc    系统内存的映射目录，提供内核与进程信息
├── root    root 用户主目录
├── sbin    存放系统二进制文件
├── srv     存放服务相关数据
├── sys     sys 虚拟文件系统挂载点
├── tmp     存放临时文件
├── usr     存放用户应用程序
└── var     存放邮件、系统日志等变化文件
```

### 一个问题

No rule to make target '/usr/lib/x86_64-linux-gnu/libGL.so'

![](https://i.loli.net/2019/09/08/G2svTNJrOc3fhei.png)

在编译Ogre的时候make install出现了这个问题，这里发现我的libGL.so指向了libGL.so.1.0.0，这是怎么一回事呢？

1. 搜索libGL.so文件路径： 比如，本机中路径为：/usr/lib/libGL.so

2. 建立symlink: sudo ln -s  /usr/lib/libGL.so.1  /usr/lib/x86_64-linux-gnu/libGL.so （之所以链接到libGL.so.1而不是libGL.so可能是为了便于区分）

3. 如果出现错误： ln: failed to create symbolic link '/usr/lib/x86_64-linux-gnu/libGL.so' : File exists

     则删除已有链接： sudo rm  /usr/lib/x86_64-linux-gnu/libGL.so

4. 重新执行步骤2建立symlink

  

### 为什么要使用动态链接库？

--《鸟哥的私房菜》中提及：动态函数库在编译的时候，在程序里面只有一个“指向”（Pointer）的位置而已，也就是说，动态函数库的内容并没有被整合到可执行文件中，而是当可执行文件要使用到函数库的时候程序才会读取函数库来使用。由于可执行文件中仅仅具有指向动态函数库所在的指标而已，并不包含函数库的内容，所以它的文件比较小一点。以下摘自APUE：

 <img src="https://i.loli.net/2019/09/13/Q1hEX6dIOHfmMSx.png" style="zoom: 50%;" />



### 如何将动态函数库加载到高速缓存中呢？

-- 1.首先，我们必须在`/etc/ld.so.conf` 文件夹里面写下想要读入高速缓存当中的动态函数库所在的目录，注意是目录而不是文件。

![](https://i.loli.net/2019/09/10/zkp5WbaRUmhlycw.png)

2.接下来利用ldconfig这个可执行文件将`/etc/ld.so.conf.d`的数据读入缓存中；

3.同时也将数据记录一份在`/etc/ld.so.cache` 这个文件当中。

Tips:可以使用`ldconfig -p`指令查看函数库内容（ld.so.cache）

### 硬连接与软连接

当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。

为解决文件的共享使用，Linux 系统引入了两种链接：硬链接 (hard link) 与软链接（又称符号链接，即 soft link 或 symbolic link）。若一个 inode 号对应多个文件名，则称这些文件为硬链接。换言之，硬链接就是同一个文件使用了多个别名。

由于硬链接是有着相同 inode 号仅文件名不同的文件，因此硬链接存在以下几点特性：

- 文件有相同的 inode 及 data block；
- 只能对已存在的文件进行创建；
- 不能交叉文件系统进行硬链接的创建；
- 不能对目录进行创建，只可对文件创建；
- 删除一个硬链接文件并不影响其他有相同 inode 号的文件。

软链接与硬链接不同，若文件用户数据块中存放的内容是另一文件的路径名的指向，则该文件就是软连接。软链接就是一个普通文件，只是数据块内容有点特殊。软链接有着自己的 inode 号以及用户数据块（见下图）。因此软链接的创建与使用没有类似硬链接的诸多限制：

- 软链接有自己的文件属性及权限等；
- 可对不存在的文件或目录创建软链接；
- 软链接可交叉文件系统；
- 软链接可对文件或目录创建；
- 创建软链接时，链接计数 i_nlink 不会增加；
- 删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接被称为死链接（即 dangling link，若被指向路径文件被重新创建，死链接可恢复为正常的软链接）。

<img src="https://i.loli.net/2019/09/10/jlnP3kGIQwpg6dm.png" style="zoom:150%;" />





## epoll

epoll:产生的事件数；及消息；使用注意

epoll 全称 eventpoll，是 linux 内核实现IO多路复用（IO multiplexing）的一个实现

epoll 监听的fd(File Descriptor)集合是常驻内核的,它有3个系统调用(*epoll_create*, *epoll_wait*, epoll_ctl)通过 *epoll_wait* 可以多次监听同一个 fd 集合，只返回可读写那部分

select 只有一个系统调用，每次要监听都要将其从用户态传到内核，有事件时返回整个集合。

从性能上看，如果 fd 集合很大，用户态和内核态之间数据复制的花销是很大的，所以 select 一般限制 fd 集合最大1024。

从使用上看，epoll 返回的是可用的 fd 子集，select 返回的是全部，哪些可用需要用户遍历判断。

尽管如此，epoll 的性能并不必然比 select 高，对于 fd 数量较少并且 fd IO 都非常繁忙的情况 select 在性能上有优势。

```c
	//之前parent和child运行在同一个session里,parent是会话（session）的领头进程,
    //parent进程作为会话的领头进程，如果exit结束执行的话，那么子进程会成为孤儿进程，并被init收养。
    //执行setsid()之后,child将重新获得一个新的会话(session)id。
    //这时parent退出之后,将不会影响到child了。
```

## 什么是session?会话

Session：在计算机中，尤其是在网络应用中，称为“会话控制”。Session对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的Web页之间跳转时，存储在Session对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当用户请求来自应用程序的 Web页时，如果该用户还没有会话，则Web服务器将自动创建一个 Session对象。当会话过期或被放弃后，服务器将终止该会话。Session 对象最常见的一个用法就是存储用户的首选项。例如，如果用户指明不喜欢查看图形，就可以将该信息存储在Session对象中。有关使用Session 对象的详细信息，请参阅“ASP应用程序”部分的“管理会话”。注意会话状态仅在支持cookie的浏览器中保留。

## 孤儿进程与僵尸进程

　孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作.

　僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。

unix提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息， 就可以得到。这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息(包括进程号the process ID,退出状态the termination status of the process,运行时间the amount of CPU time taken by the process等)。直到父进程通过wait / waitpid来取时才释放。 但这样就导致了问题，**如果进程不调用wait / waitpid的话，** **那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程. 此即为僵尸进程的危害，应当避免。**

　**孤儿进程是没有父进程的进程，孤儿进程这个重任就落到了init进程身上**，init进程就好像是一个民政局，专门负责处理孤儿进程的善后工作。每当出现一个孤儿进程的时候，内核就把孤 儿进程的父进程设置为init，而init进程会循环地wait()它的已经退出的子进程。这样，当一个孤儿进程凄凉地结束了其生命周期的时候，init进程就会代表党和政府出面处理它的一切善后工作。**因此孤儿进程并不会有什么危害。**

**任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。**这是每个 子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。  如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。

## 僵尸进程危害场景：

　　例如有个进程，它定期的产 生一个子进程，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，父进程只管生成新的子进程，至于子进程 退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程，倘若用ps命令查看的话，就会看到很多状态为Z的进程。 严格地来说，僵死进程并不是问题的根源，罪魁祸首是产生出大量僵死进程的那个父进程。因此，当我们寻求如何消灭系统中大量的僵死进程时，答案就是把产生大 量僵死进程的那个元凶枪毙掉（也就是通过kill发送SIGTERM或者SIGKILL信号啦）。枪毙了元凶进程之后，它产生的僵死进程就变成了孤儿进 程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源，这样，这些已经僵死的孤儿进程 就能瞑目而去了。



需要注意的是，用户层守护进程的父进程是 init进程（进程ID为1），从上面的输出`PPID`一列也可以看出，内核守护进程的父进程并非是 init进程。对于用户层守护进程， 因为它真正的父进程在 fork 出子进程后就先于子进程 exit 退出了，所以它是一个由 init 继承的孤儿进程。

进程组 ：

- 每个进程除了有一个进程ID之外，还属于一个进程组
- 进程组是一个或多个进程的集合，同一进程组中的各进程接收来自同一终端的各种信号
- 每个进程组有一个组长进程。组长进程的进程组ID等于其进程ID

会话：会话（session）是一个或多个进程组的集合，进程调用 setsid 函数（原型：`pid_t setsid(void)` ）建立一个会话。

  进程调用 setsid 函数建立一个新会话，如果调用此函数的进程不是一个进程组的组长，则此函数创建一个新会话。具体会发生以下3件事：

- 该进程变成新会话的会话首进程（session leader，会话首进程是创建该会话的进程）。此时，该进程是新会话的唯一进程。
- 该进程成为一个新进程组的组长进程。新进程组ID是该调用进程的进程ID
- 该进程没有控制终端。如果调用setsid之前该进程有一个控制终端，那么这种联系也被切断

如果该调用进程已经是一个进程组的组长，则此函数返回出错。为了保证不处于这种情况，通常先调用fork，然后使其父进程终止，而子进程则继续。因为子进程继承了父进程的进程组ID，而其进程ID是重新分配的，两者不可能相等，这就保证了子进程不是一个进程组的组长。

## **创建守护进程的过程：**  

\1. 调用fork创建子进程。父进程终止，让子进程在后台继续执行。
\2. 子进程调用setsid产生新会话期并失去控制终端调用setsid()使子进程进程成为新会话组长和新的进程组长，同时失去控制终端。
\3. 忽略SIGHUP信号。会话组长进程终止会向其他进程发该信号，造成其他进程终止。
\4. 调用fork再创建子进程。子进程终止，子子进程继续执行，由于子子进程不再是会话组长，从而禁止进程重新打开控制终端。
\5. 改变当前工作目录为根目录。一般将工作目录改变到根目录，这样进程的启动目录也可以被卸掉。
\6. 关闭打开的文件描述符，打开一个空设备，并复制到标准输出和标准错误上。 避免调用的一些库函数依然向屏幕输出信息。
\7. 重设文件创建掩码清除从父进程那里继承来的文件创建掩码，设为0。
\8. 用openlog函数建立与syslogd的连接。

/dev/null是一个特殊的设备文件，这个文件接收到的任何数据都会被丢弃。因此，null这个设备通常也被成为位桶（bit bucket）或黑洞。

## 什么是 Event Loop？

## **shared_ptr**?

智能指针的原理是:接受一个申请好的内存地址,构造一个保存在栈上的智能指针对象,当程序退出栈的作用域范围后,由于栈上的变量自动被销毁,智能指针内部保存的内存也就被释放掉了(除非将智能指针保存起来)

C11提供了三种智能指针:std::shared_ptr,  std::unique_ptr,   std::weak_ptr  使用时需要包含头文件<memory>

shared_ptr使用引用计数，每一个shared_ptr的拷贝都指向相同的内存。每使用他一次，内部的引用计数加1，每析构一次，内部的引用计数减1，减为0时，删除所指向的堆内存。shared_ptr内部的引用计数是安全的，但是对象的读取需要加锁。

文件的写入也是如此，拿到offet，调用实际的写入方法，最后更新offset。到此为止一个文件的读和写的大体过程我们是清楚了，很显然上述的过程并不是原子的，无论是文件的读还是写，都至少有两个步骤，一个是拿offset，另外一个则是实际的读和写。并且在整个过程中并没有看到加锁的动作，那么第一个问题就得到了解决。对于第二个问题我们可以简要的分析下，假如有两个线程，第一个线程拿到offset是1，然后开始写入，在写入的过程中，第二个线程也去拿offset，因为对于一个文件来说多个线程是共享同一个struct file结构，因此拿到的offset仍然是1，这个时候线程1写结束，更新offset，然后线程2开始写。最后的结果显而易见，线程2覆盖了线程1的数据，通过分析可知，多线程写文件不是原子的，会产生数据覆盖。但是否会产生数据错乱，也就是数据交叉写入了?其实这种情况是不会发生的，至于为什么请看下面这段代码:

```c
ssize_t generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
        unsigned long nr_segs, loff_t pos)
{
    struct file *file = iocb->ki_filp;
    struct inode *inode = file->f_mapping->host;
    struct blk_plug plug;
    ssize_t ret;

    BUG_ON(iocb->ki_pos != pos);
    // 文件的写入其实是加锁的
    mutex_lock(&inode->i_mutex);
    blk_start_plug(&plug);
    ret = __generic_file_aio_write(iocb, iov, nr_segs, &iocb->ki_pos);
    mutex_unlock(&inode->i_mutex);

    if (ret > 0 || ret == -EIOCBQUEUED) {
        ssize_t err;

        err = generic_write_sync(file, pos, ret);
        if (err < 0 && ret > 0)
            ret = err;
    }
    blk_finish_plug(&plug);
    return ret;
}
EXPORT_SYMBOL(generic_file_aio_write);

```

所以并不会产生数据错乱，只会存在数据覆盖的问题，既然如此我们在实际的进行文件读写的时候是否需要进行加锁呢? 加锁的确是可以解决问题的，但是在这里未免有点牛刀杀鸡的感觉，好在OS给我们提供了原子写入的方法，第一种就是在打开文件的时候添加O_APPEND标志，通过O_APPEND标志将获取文件的offset和文件写入放在一起用锁进行了保护，使得这两步是原子的，具体代码可以看上面代码中的__generic_file_aio_write函数。

```c
ssize_t __generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
                 unsigned long nr_segs, loff_t *ppos)
{
    struct file *file = iocb->ki_filp;
    struct address_space * mapping = file->f_mapping;
    size_t ocount;      /* original count */
    size_t count;       /* after file limit checks */
    struct inode    *inode = mapping->host;
    loff_t      pos;
    ssize_t     written;
    ssize_t     err;

    ocount = 0;
    err = generic_segment_checks(iov, &nr_segs, &ocount, VERIFY_READ);
    if (err)
        return err;

    count = ocount;
    pos = *ppos;

    vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);

    /* We can write back this queue in page reclaim */
    current->backing_dev_info = mapping->backing_dev_info;
    written = 0;
    // 重点就在这个函数
    err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
    if (err)
        goto out;
    ......// 省略
}

inline int generic_write_checks(struct file *file, loff_t *pos, size_t *count, int isblk)
{
    struct inode *inode = file->f_mapping->host;
    unsigned long limit = rlimit(RLIMIT_FSIZE);

        if (unlikely(*pos < 0))
                return -EINVAL;

    if (!isblk) {
        /* FIXME: this is for backwards compatibility with 2.4 */
        // 如果带有O_APPEND标志，会直接拿到文件的大小，设置为新的offset
        if (file->f_flags & O_APPEND)
                        *pos = i_size_read(inode);

        if (limit != RLIM_INFINITY) {
            if (*pos >= limit) {
                send_sig(SIGXFSZ, current, 0);
                return -EFBIG;
            }
            if (*count > limit - (typeof(limit))*pos) {
                *count = limit - (typeof(limit))*pos;
            }
        }
    }
    ......// 省略
}
```



最后一个问题是多个进程写同一个文件是否会造成文件写错乱，直观来说是多进程写文件不是原子的，这是很显而易见的，因为每个进程都拥有一个struct file对象，是独立的，并且都拥有独立的文件offset，所以很显然这会导致上文中说到的数据覆盖的情况，但是否会导致数据错乱呢?，答案是不会，虽然struct file对象是独立的，但是struct inode是共享的(相同的文件无论打开多少次都只有一个struct inode对象)，文件的最后写入其实是先要写入到页缓存中，而页缓存和struct inode是一一对应的关系，在实际文件写入之前会加锁，而这个锁就是属于struct inode对象(见上文中的mutex_lock(&inode->i_mutex))的，所有无论有多少个进程或者线程，只要是对同一个文件写数据，拿到的都是同一把锁，是线程安全的，所以也不会出现数据写错乱的情况。

### `write`：不会出现数据交叉的情况，而且父子进程交替执行写入。

从上面小节的测试过程可以发现，两个非亲缘关系的进程同时写一个文件时，会出现数据混乱的情况，但是两个进程写入的数据没有覆盖。

这是因为这两个进程表项中指向的对应的两个文件表项对应的当前文件偏移量是不一致的，但是由于打开文件时是使用append追加的方式，使得进程指向的文件表项中的当前文件偏移量都等于当前文件中所有数据的总长度。这就是为什么写入的数据会出现错乱，但是不会出现覆盖（偏移量不一致）的原因。

注意：内核write函数在写入时是`原子`操作，所以两个进程会有一个竞争关系，最终只会由某个进程写入数据。

为什么fwrite会出问题? ，因为有buf，所以就会有问题?，这块还没有讲解清楚哦，还是需要分析下的，另外write为什么不会错乱? 分析不够到位啊，我之前也写过一篇文章分析过这个问题，http://blog.csdn.net/zhangyifei216/article/details/76653746，fwrite的问题说白了其实就是用户态buf，会缓存多次写入，然后再一次性调用底层的write，所以给你的错觉就是多次写入之间交叉了(个人理解)

class ChatServer final  说明class ChatServer 不能被别人继承了

~ChatServer() = default; //c++11 类默认函数的控制："=default" 和 "=delete"函数

```c
C++ 的类有四类特殊成员函数，它们分别是：默认构造函数、析构函数、拷贝构造函数以及拷贝赋值运算符。
这些类的特殊成员函数负责创建、初始化、销毁，或者拷贝类的对象。
如果程序员没有显式地为一个类定义某个特殊成员函数，而又需要用到该特殊成员函数时，则编译器会隐式的为这个类生成一个默认的特殊成员函数。// C++11 标准引入了一个新特性："=default"函数。程序员只需在函数声明后加上“=default;”，就可将该函数声明为 "=default"函数，编译器将为显式声明的 "=default"函数自动生成函数体。
//该函数比用户自己定义的默认构造函数获得更高的代码效率
// "=default"函数特性仅适用于类的特殊成员函数，且该特殊成员函数没有默认参数。
class X1
{
public:
    int f() = default;      // err , 函数 f() 非类 X 的特殊成员函数
    X1(int, int) = default;  // err , 构造函数 X1(int, int) 非 X 的特殊成员函数
    X1(int = 1) = default;   // err , 默认构造函数 X1(int=1) 含有默认参数
};
// 为了能够让程序员显式的禁用某个函数，C++11 标准引入了一个新特性："=delete"函数。程序员只需在函数声明后上“=delete;”，就可将该函数禁用。
```

*POD* stands for *Plain Old Data* - that is, a class (whether defined with the keyword `struct` or the keyword `class`) without constructors, destructors and virtual members functions. [Wikipedia's article on POD](http://en.wikipedia.org/wiki/Plain_Old_Data_Structures) goes into a bit more detail and defines it as:

> A Plain Old Data Structure in C++ is an aggregate class that contains only PODS as members, has no user-defined destructor, no user-defined copy assignment operator, and no nonstatic members of pointer-to-member type.

A POD is a type (including classes) where the C++ compiler guarantees that there will be no "magic" going on in the structure: for example hidden pointers to vtables, offsets that get applied to the address when it is cast to other types (at least if the target's POD too), constructors, or destructors. Roughly speaking, a type is a POD when the only things in it are built-in types and combinations of them. The result is something that "acts like" a C type.

explicit

使用&命令后，作业被提交到后台运行，当前控制台没有被占用，但是一但把当前控制台关掉(退出帐户时)，作业就会停止运行。nohup命令可以在你退出帐户之后继续运行相应的进程。nohup就是不挂起的意思( no hang up)。该命令的一般形式为：
nohup command &
1
如果使用nohup命令提交作业，那么在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中，除非另外指定了输出文件

//注册协议

//http://47.97.25.88:12345/register.do?p={"username": "13917043329", "nickname": "balloon", "password": "123"}

//{"code": 0, "msg" : "ok"}

## 大作业:基于B+树的数据库索引引擎

目标：完成支持整数为key的数据库索引功能，提供包括insert, del, update的多线程事务操作。
形式要求：基于C/C++，提供上述接口，并封装为SO库。
实现约束：所有索引文件需统一存储在一个大文件中，索引的数据可以为任意数据结构; 需支持多线程环境，采用多生产者多消费者模型；完成持续读写压力测试，运行时间不低于12小时，插入对象不低于1千万个，观察CPU，内存，磁盘开销，并将压测分析输出到报告中。
提交内容：可编译的源码，可在Linux环境中运行的程序；设计文档（包括模块框架设计，重要流程图（主要是insert,del,update接口），核心数据结构，运行时截图（操作相关结果）。
完成时间：11.20 日前提交，提交实验报告及源代码。
提交邮箱： lpue2014@163.com

select * from db where id=838;int temp = sscanf(mp->cmd,"select * from db where id=%d;\n", keyIndex); 1

总结：之前写都是什么(*Pointer).member,这么写太麻烦了,所以直接用箭头来省略部分书写内容
**箭头（->）：左边必须为指针；**
**点号（.）：左边必须为实体**

acewzj is not in the sudoers file

```c++
// int to key_t
void intToKeyT(bpt::key_t *a,int *b){
	char key[16] = { 0 };//如果只对数组的一部分进行初始化，则编译器将把其他元素设置为0。因此，当只将第一个元素初始化为0时，编译器会自动将其他元素设为0 (就像前一句说的那样)。或者3.用memset函数在程序开始时初始化数组。比如：int arr[1024];memset(arr, 0, 1024); //清零
	sprintf(key, "%d", *b);
    ///* Write formatted output to S.  */
	//extern int sprintf (char *__restrict __s,   //
	//	    const char *__restrict __format, ...) __THROWNL;
	*a = key;
}
```

__restrict关键字告诉编译器额外信息(两个指针不指向同一数据),从而生成更优化的[汇编代码](https://en.cppreference.com/w/c/language/restrict).

__THROW is meant to declare the function as capable of
throwing exceptions (a C++ feature). In C, the macro does nothing.

```c++

```

```c++
int return_code = searchRecord(duck_db_ptr,keyIndex,return_val);
//duck_db_ptr 
// search by index
int searchRecord(bplus_tree *treePtr,int *index, value_t *return_val){
    bpt::key_t key;
    intToKeyT(&key,index);
    //key is key_t ype of char k[16];there is to convert index [int] -> key_t
	return (*treePtr).search(key, return_val); 
}
int bplus_tree::search(const key_t& key, value_t *value) const
{
	leaf_node_t leaf;
    //叶子节点包括--
			/* leaf node block */
            struct leaf_node_t {
                typedef record_t *child_t;
                off_t parent; /* parent node offset */
                off_t next;//双向指针
                off_t prev;//双向指针
                size_t n;//
                record_t children[BP_ORDER];
            };
    
    map(&leaf, search_leaf(key));
    	    off_t search_leaf(const key_t &key) const
            {
                return search_leaf(search_index(key), key);
            }
    //根据key找出index?-->key只是key,index
                    off_t bplus_tree::search_index(const key_t &key) const
                {
                    off_t org = meta.root_offset;
                    int height = meta.height;
                        //root不是第一个元素,肯定是居中的元素,所以需要记录一下root 的偏移记录
                        //meta
                        /* meta information of B+ tree */
                        typedef struct {
                            size_t order; /* `order` of B+ tree */阶?有50个子节点
                            size_t value_size; /* size of value */  516=256+256+4
                            size_t key_size;   /* size of key */    16
                            size_t internal_node_num; /* how many internal nodes */8个内部节点
                            size_t leaf_node_num;     /* how many leafs */  192个叶子节点
                            size_t height;            /* height of tree (exclude leafs) */2
                            off_t slot;        /* where to store new block */
                            off_t root_offset; /* where is the root of internal nodes */
                            off_t leaf_offset; /* where is the first leaf */
                        } meta_t;
                        
                        
                    while (height > 1) {
                        internal_node_t node;
                        map(&node, org);
                            template<class T>
                                int map(T *block, off_t offset) const
                            {
                                return map(block, offset, sizeof(T));
                            }
                            /* read block from disk */
                                int map(void *block, off_t offset, size_t size) const
                                {
                                    open_file();
                                    fseek(fp, offset, SEEK_SET);
                                    size_t rd = fread(block, size, 1, fp);
                                    close_file();

                                    return rd - 1;
                                }
                        
                        index_t *i = upper_bound(begin(node), end(node) - 1, key);
                        
                        
                        
                        
                        org = i->child;
                        --height;
                    }

                    return org;
                }
```

#### fread:

```c++
size_t fread(void *ptr, size_t size, size_t nmemb, FILE *stream)
ptr -- 这是指向带有最小尺寸 size*nmemb 字节的内存块的指针。
size -- 这是要读取的每个元素的大小，以字节为单位。
nmemb -- 这是元素的个数，每个元素的大小为 size 字节。
stream -- 这是指向 FILE 对象的指针，该 FILE 对象指定了一个输入流。
返回值
```

search_index只是要找到key所对应的偏移地址--怎么找到呢?

首先找到根的偏移,然后取出根的偏移在文件中的数据,有偏移[起始地址]+数据类型大小[long 8]bit parent next prev n children  ,相当于反序列化?从磁盘上复活数据对象哈哈,

接着根据这个数据,因为数据里面有一个数组[50阶]的,从这里面找到比key要大的那个.这是因为在节点里面数组从左到右依次增大.

接着一层一层的往下找它的孩子?它的孩子里面存着是

update db 111 23 123@163.com where id=1;

select * from db where id in(0,9);

insert db 5002 acej 25 acewzj@qq.com;

delete from db where id=2; 

在 B+树中删除关键字时，有以下几种情况：

2、 当删除某结点中最大或者最小的关键字，就会涉及到更改其双亲结点一直到根结点中所有索引值的更改。

例如，在图 1的 B+树中删除关键字 97，删除后的 B+树如图 6 所示：![](http://data.biancheng.net/uploads/allimg/171024/2-1G024141030209.png)

当删除该关键字，导致当前结点中关键字个数小于`⌈M/2⌉`
，若其兄弟结点中含有多余的关键字，可以从兄弟结点中借关键字完成删除操作。例如，在图 1 的 B+树中删除关键字 51，由于其兄弟结点中含有 3 个关键字，所以可以选择借一个关键字，同时修改双亲结点中的索引值，删除之后的 B+树如图 7 所示：

![](http://data.biancheng.net/uploads/allimg/171024/2-1G024141129106.png)

#### 多线程

0 begin wait a condition...
1 begin wait a condition...
2 begin wait a condition...

0 begin produce product...
produce 86
0 end produce product...

- **以默认方式启动的线程，在线程结束后不会自动释放占有的系统资源，要在主控线程中调用pthread_join()后才会释放**
- **以分离状态启动的线程，在线程结束后会自动释放所占有的系统资源,这个时候就不需要调用pthread_join方法了**

## **Segmentation fault**

所谓的段错误就是指访问的内存超过了系统所给这个程序的内存空间，通常这个值是由gdtr来保存的，他是一个48位的寄存器，其中的32位是保存由它指向的gdt表，后13位保存相应于gdt的下标，最后3位包括了程序是否在内存中以及程序的在cpu中的运行级别,指向的gdt是由以64位为一个单位的表，在这张表中就保存着程序运行的代码段以及数据段的起始地址以及相应的断限和页面交换还有程序运行级别和内存粒度等信息，一旦一个程序发生了越界访问，CPU就会产生相应的异常保护，于是segmentation fault就出现了。即“当程序试图访问不被允许访问的内存区域（比如，尝试写一块属于操作系统的内存），或以错误的类型访问内存区域（比如，尝试写一块只读内存）。这个描述是准确的。为了加深理解，我们再更加详细的概括一下SIGSEGV。段错误应该就是访问了不可访问的内存，这个内存要么是不存在的，要么是受系统保护的。

指针越界和SIGSEGV是最常出现的情况，经常看到有帖子把两者混淆，而这两者的关系也确实微妙。在此，我们把指针运算（加减）引起的越界、野指针、空指针都归为指针越界。SIGSEGV在很多时候是由于指针越界引起的，但并不是所有的指针越界都会引发SIGSEGV。一个越界的指针，如果不引用它，是不会引起SIGSEGV的。而即使引用了一个越界的指针，也不一定引起SIGSEGV。这听上去让人发疯，而实际情况确实如此。SIGSEGV涉及到操作系统、C库、编译器、链接器各方面的内容，我们以一些具体的例子来说明。

（1）错误的访问类型引起

```c++
#include<stdio.h>
#include<stdlib.h>

int main(){
    char *c = "hello world";
    c[1] = 'H';
}
```

上述程序编译没有问题，但是运行时弹出SIGSEGV。此例中，”hello world”作为一个常量字符串，在编译后会被放在.rodata节（GCC），最后链接生成目标程序时.rodata节会被合并到text segment与代码段放在一起，故其所处内存区域是只读的。这就是错误的访问类型引起的SIGSEGV。

上述程序编译没有问题，但是运行时弹出SIGSEGV。此例中，”hello world”作为一个常量字符串，在编译后会被放在.rodata节（GCC），最后链接生成目标程序时.rodata节会被合并到text segment与代码段放在一起，故其所处内存区域是只读的。这就是错误的访问类型引起的SIGSEGV。

（2）访问了不属于进程地址空间的内存

```c
#include <stdio.h> 
#include <stdlib.h>

int main(){ 
    int* p = (int*)0xC0000fff; 
    *p = 10; 
}　
```

还有一种可能，往受到系统保护的内存地址写数据，最常见的就是给一个指针以0地址；

还有一种可能，往受到系统保护的内存地址写数据，最常见的就是给一个指针以0地址；

```c
int  i=0; 
scanf ("%d", i);  /* should have used &i */ 
printf ("%d\n", i);
return 0;　
```

（3）访问了不存在的内存
最常见的情况不外乎解引用空指针了，如：

（3）访问了不存在的内存
最常见的情况不外乎解引用空指针了，如：

```c
int *p = null;
*p = 1;
```

在实际情况中，此例中的空指针可能指向用户态地址空间，但其所指向的页面实际不存在。

（4）内存越界，数组越界，变量类型不一致等

```c
#include <stdio.h>

int  main(){ 
        char test[1]; 
        printf("%c", test[10]); 
        return 0; 
}　
```

这就是明显的数组越界了，或者这个地址根本不存在。

这就是明显的数组越界了，或者这个地址根本不存在。

（5）试图把一个整数按照字符串的方式输出

```c
int  main() { 
    int b = 10; 
    printf("%s\n", b);
    return 0; 
}　
```

这是什么问题呢？由于还不熟悉调试动态链接库，所以我只是找到了printf的源代码的这里。

这是什么问题呢？由于还不熟悉调试动态链接库，所以我只是找到了printf的源代码的这里。

```c
声明部分：
   int pos =0 ,cnt_printed_chars =0 ,i ;
　　unsigned char *chptr ;
　　va_list ap ;
%s格式控制部分：
case 's':
　　    chptr =va_arg (ap ,unsigned char *);
　　    i =0 ;
　　    while (chptr [i ])
　　    {...
　　        cnt_printed_chars ++;
　　        putchar (chptr [i ++]);
　　}
```

仔细看看，发现了这样一个问题，在打印字符串的时候，实际上是打印某个地址开始的所有字符，但是当你想把整数当字符串打印的时候，这个整数被当成了一个地址，然后printf从这个地址开始去打印字符，直到某个位置上的值为\0。所以，如果这个整数代表的地址不存在或者不可访问，自然也是访问了不该访问的内存——segmentation fault。
 类似的，还有诸如：sprintf等的格式控制问题，比如，试图把char型或者是int的按照%s输出或存放起来，如：

仔细看看，发现了这样一个问题，在打印字符串的时候，实际上是打印某个地址开始的所有字符，但是当你想把整数当字符串打印的时候，这个整数被当成了一个地址，然后printf从这个地址开始去打印字符，直到某个位置上的值为\0。所以，如果这个整数代表的地址不存在或者不可访问，自然也是访问了不该访问的内存——segmentation fault。
 类似的，还有诸如：sprintf等的格式控制问题，比如，试图把char型或者是int的按照%s输出或存放起来，如：

```c
#include <stdio.h>
#include <string.h>
char c='c';
int i=10;
char buf[100];
printf("%s", c);        //试图把char型按照字符串格式输出，这里的字符会解释成整数，再解释成地址，所以原因同上面那个例子
printf("%s", i);            //试图把int型按照字符串输出
memset(buf, 0, 100);
sprintf(buf, "%s", c);    //试图把char型按照字符串格式转换
memset(buf, 0, 100);
sprintf(buf, "%s", i);   //试图把int型按照字符串转换
```

（6）栈溢出了，有时SIGSEGV，有时却啥都没发生
大部分C语言教材都会告诉你，当从一个函数返回后，该函数栈上的内容会被自动“释放”。“释放”给大多数初学者的印象是free()，似乎这块内存不存在了，于是当他访问这块应该不存在的内存时，发现一切都好，便陷入了深深的疑惑。

防止Segmentation fault的出现需要注意：

定义了指针以后记得初始化，在使用的时候记得判断是否为NULL；
在使用数组的时候是否被初始化，数组下标是否越界，数组元素是否存在等；
在变量处理的时候变量的格式控制是否合理等；

## 十字符病毒

## SHELL

![image-20191225114928880](https://i.loli.net/2020/03/29/eBJXV7INS8iyxgU.png)

![image-20191225120846398](https://i.loli.net/2020/03/30/NC1otVh5ZeDjTUa.png)



此外，还发现第一个进程占用很大cpu资源，就是名为apgffcztwi的进程，这个进程名刚好10个字符，这是什么进程，名字相当古怪，肯定有问题，从文件名看出，这不像一个正常的系统进程。