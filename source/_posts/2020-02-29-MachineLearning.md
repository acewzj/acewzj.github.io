---
title:  "Machine_Learning"
date:   2020-02-29 10:16:18 +0800
categories:
- C++
tags: 
- ML 
---

这篇文章主要记述了机器学习的一些笔记。
<!--more-->

与此有关联的任务是降维，降维的目的是简化数据、但是不能失去大部分信息。做法之一是合并若干相关的特征。例如，汽车的里程数与车龄高度相关，降维算法就会将它们合并成一个，表示汽车的磨损。这叫做特征提取。


# 欠拟合与过拟合

西瓜书：我们实际希望的，是在新样本上能表现的很好的学习器。为了达到这个目的，应该从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”，这样才能在遇到新样本时做出正确的判别，然而，当学习器把训练样本学得“太好”了的时候，很可能把训练样本本身的一些特点当作了所有潜在样本都会具有的一般性质，这样会导致泛化性能下降。欠拟合则是指对训练样本的一般性质没学好，它比较容易克服，例如通过在决策树学习中扩展分支，在神经网络学习中增加训练次数等。过拟合则比较麻烦，是ML面临的关键障碍，它是无法彻底避免的，我们能做的只是“缓解”，或者说是减少其风险。

![1580776286554](https://i.loli.net/2020/03/29/7vzmD4AsSI21XBC.png)

如上图所示：用一次函数进行欠拟合`underfitting`，会出现数据点都已经平缓了但是函数曲线依旧上升的错误结果，这是先入为主的，具有高偏差且不能预测房子的价格。

而用第三幅图的高次函数进行过拟合`overfitting`，可以使得整体的偏差函数J达到最小，毕竟将每个点都紧密的连在了一起，具有高方差但是这样起起伏伏的曲线也不能预测未来的房子的价格走势。

![1580777298194](https://i.loli.net/2020/03/29/YTe8ALcOmpyIqsf.png)

但是，如果我们有过多的变量，而只有很少的训练数据时，（比如我们预测房价时感觉房子面积、卧室数量、厨房面积等都和房价有关系）就会出现过度拟合的问题。【如果训练数据多的话，每个点挨得密密的，那么就可以不用进行过度波动了，也就可以进行预测房价走势了】

为了减少过拟合，有两个方法：一个是减少选取变量的数量或者就选择重要的变量；另外一个是正则化。

![1580789332698](https://i.loli.net/2020/03/29/mvQfCiq2AwxU5Y7.png)

正则化的意思就是说既保留特征变量，又通过使特征变量前面的参数变得微不足道来避免过拟合，lambda就是用来控制这两个参数之间的平衡关系trade-off，即更好的去拟合训练集的目标和将参数控制的更小的目标。如果对参数的惩罚（penalize）过大会导致参数趋近于0，导致最后只有一个参数，这就是欠拟合的例子。

> 这里的1/2m是用来减少值，m是样本容量，只是为了使数学更直白一点。

# 梯度下降

**总而言之，权重告诉你这个第二层的神经元关注什么样的像素图案，偏置（bias）则告诉你加权和得有多大，才能让神经元的激发变得有意义。**

![1582623473821](https://i.loli.net/2020/03/29/qylHXtrwP93u6Jc.png)

你要将每个垃圾（错误的）输出激活值与你想要的值之间的差的平方加起来，【正确的话就是减1，错误的话就是减0，这样的话错误的输出会得到很高的惩罚】这就是我们称之为训练单个样本的“代价”

![1582624671730](https://i.loli.net/2020/03/29/uUPhJgEwkcsnpFq.png)

![1582625328359](https://i.loli.net/2020/03/29/Ooz2DRM78lgXqBf.png)

![ ](17-星期一-95202.gif)

![1580955304837](https://i.loli.net/2020/03/29/5Am4zSWByTnMotN.png)

![1580956150366](https://i.loli.net/2020/03/29/V25CKsaX6ZzDjYm.png)

![1580957499820](https://i.loli.net/2020/04/03/Bk64R9fudU1nyYW.png)

![1580957706734](https://i.loli.net/2020/03/29/dEYA6uejGnKWvqF.png)

![1580971811886](https://i.loli.net/2020/03/29/LE7gT2WxBnAkqfz.png)

![1581130404619](https://i.loli.net/2020/03/29/Fikfzs6RbeYrnox.png)

# SVM

有的二维数据不能很好的用SVM超平面进行分类，就说它是线性不可分的。如何做呢？可以将它进行高维映射，但是怎么选取映射函数是很难的，不如直接就选择核函数进行映射吧还简单。

![image-20200318094304398](https://i.loli.net/2020/03/29/rxU2Nl5OWPZetCB.png)

![1581583849931](https://i.loli.net/2020/03/29/IZBxtCjfuv74b93.png)

![1581600347364](https://i.loli.net/2020/03/29/4EVvsPWGBr2ZhI8.png)

均匀分布比起高斯分布具有更高的信息熵（更混乱，因为均匀分布说明物体很多且分布均匀，而高斯分布集中在峰值那块，更整齐一些）

![1583995920661](https://i.loli.net/2020/03/30/lY8G7mgMEnroPa2.png)

![1583996284858](https://i.loli.net/2020/03/30/zXL9xRYIdmP6SEO.png)

第二个式子是保证正负号的，也就是保证分类正确（因为求w最小求出两个值）

![1583998459306](https://i.loli.net/2020/03/30/gMdYEFTuqoCByi1.png)



![绿色代表正值，红色代表负值](https://i.loli.net/2020/03/29/7AVORCfeqYQ6zkN.png)



![](https://i.loli.net/2020/03/29/lTWn82jYoUzV1Lg.png)

![1583391636661](https://i.loli.net/2020/03/29/nCdycNZARDSOKgH.png)

![1583392810592](https://i.loli.net/2020/03/29/SVjBok6dvct7puP.png)

![1583394660258](https://i.loli.net/2020/03/29/gx7pl4i9XhqKVFz.png)

### KNN优劣

![1583809914745](https://i.loli.net/2020/03/29/UQWJvuoxsGLXEF2.png)

# 正则化的作用

二范式可以让参数的值更平滑。而一范数是绝对值，图像就是有棱有角的，两个参数中一定有一个为0，所以使得参数更稀疏。

![1583808319588](https://i.loli.net/2020/04/06/tNWnDselqB9S7vY.png)

正则化是为了惩罚

![1583394985712](https://i.loli.net/2020/03/29/vzktO3gA1yxCWcD.png)

# 卷积

![1582616523275](https://i.loli.net/2020/03/29/PbpsBN4MKq3QEnO.png)

![绿色代表正值，红色代表负值](https://i.loli.net/2020/04/06/AInw2qafe4gUd5P.png)